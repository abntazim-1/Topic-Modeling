{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TAZIM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NMF topic modeling pipeline...\n",
      "Loaded 2225 rows. Using text column: content\n",
      "Columns: ['category', 'filename', 'title', 'content']\n",
      "Cleaning raw text...\n",
      "Lemmatizing with spaCy (this can take a while)...\n",
      "Building bigrams/trigrams...\n",
      "Dictionary size after filtering: 5834\n",
      "\n",
      "=== TF-IDF max_features = 2000 ===\n",
      "TF-IDF shape: (2225, 2000)\n",
      "Training NMF: topics=4, params={'beta_loss': 'frobenius', 'alpha': 0.0, 'l1_ratio': 0.0}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NMF.__init__() got an unexpected keyword argument 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 282\u001b[39m\n\u001b[32m    279\u001b[39m test_docs = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Set to a small number like 100 for quick testing\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting NMF topic modeling pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m best_model, results, output_file = \u001b[43mrun_nmf_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_run_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_docs\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    289\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36mrun_nmf_pipeline\u001b[39m\u001b[34m(csv_path, text_col, out_dir, test_run_docs, tfidf_max_features_options, n_topic_range, nmf_kwargs_grid)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining NMF: topics=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_topics\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, params=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnmf_kw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# create NMF with given params\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m nmf = \u001b[43mNMF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnndsvda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnmf_kw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNMF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Note: sklearn will ignore unknown params in the dict if not valid for NMF; we pass common ones\u001b[39;00m\n\u001b[32m    189\u001b[39m W = nmf.fit_transform(X)\n",
      "\u001b[31mTypeError\u001b[39m: NMF.__init__() got an unexpected keyword argument 'alpha'"
     ]
    }
   ],
   "source": [
    "# nmf_improved_pipeline.py\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"ner\", \"parser\"])  # heavy model for good lemmatization\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities & Preprocessing\n",
    "# ---------------------------\n",
    "NEWS_STOPWORDS = {\n",
    "    \"said\",\"say\",\"mr\",\"mrs\",\"ms\",\"one\",\"two\",\"new\",\"news\",\"bbc\",\"also\",\n",
    "    \"would\",\"could\",\"like\",\"year\",\"years\",\"today\",\"last\",\"first\",\"may\",\"told\",\n",
    "    \"reuters\",\"report\",\"reports\",\"said\",\"u\",\"s\",\"us\"\n",
    "}\n",
    "\n",
    "# Cleaner function\n",
    "def clean_text_raw(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)     # URLs\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)                   # emails\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)                 # non-letters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Full preprocessing: lemmatize & POS filter\n",
    "def spacy_lemmatize(texts, allowed_pos={\"NOUN\",\"ADJ\",\"VERB\",\"ADV\"}, batch_size=50):\n",
    "    stop_words = set(stopwords.words(\"english\")).union(NEWS_STOPWORDS)\n",
    "    tokenized = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        toks = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha and (token.pos_ in allowed_pos):\n",
    "                lemma = token.lemma_.strip()\n",
    "                if lemma and lemma not in stop_words and len(lemma) > 2:\n",
    "                    toks.append(lemma)\n",
    "        tokenized.append(toks)\n",
    "    return tokenized\n",
    "\n",
    "# Build bigrams/trigrams\n",
    "def make_ngrams(tokenized_texts, min_count=5, threshold=50):\n",
    "    bigram = Phrases(tokenized_texts, min_count=min_count, threshold=threshold)\n",
    "    trigram = Phrases(bigram[tokenized_texts], threshold=threshold)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "    texts_bi = [bigram_mod[doc] for doc in tokenized_texts]\n",
    "    texts_tri = [trigram_mod[bigram_mod[doc]] for doc in texts_bi]\n",
    "    return texts_tri, bigram_mod, trigram_mod\n",
    "\n",
    "# Convert token lists to joined strings (for TF-IDF)\n",
    "def join_tokens(texts):\n",
    "    return [\" \".join(t) for t in texts]\n",
    "\n",
    "# Extract top words for an NMF component\n",
    "def top_words_from_components(H, feature_names, topn=12):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_indices = topic.argsort()[:-topn-1:-1]\n",
    "        topics.append([feature_names[i] for i in top_indices])\n",
    "    return topics\n",
    "\n",
    "# Wordcloud for a list of words (weights optional)\n",
    "def plot_wordcloud(words, title=None, savepath=None):\n",
    "    # words can be list or dict (word->weight)\n",
    "    if isinstance(words, list):\n",
    "        freq = {w: (len(w) + 1) for w in words}  # naive weight\n",
    "    elif isinstance(words, dict):\n",
    "        freq = words\n",
    "    else:\n",
    "        freq = {w: 1 for w in words}\n",
    "    wc = WordCloud(width=600, height=350, background_color=\"white\").generate_from_frequencies(freq)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------\n",
    "def run_nmf_pipeline(\n",
    "    csv_path=r\"G:\\Topic Modeling Project\\artifacts\\bbc-news-data.csv\",\n",
    "    text_col=\"content\",\n",
    "    out_dir=\"nmf_results\",\n",
    "    test_run_docs=None,\n",
    "    tfidf_max_features_options=[2000, 3000],\n",
    "    n_topic_range=range(4,13),\n",
    "    nmf_kwargs_grid=[{\"beta_loss\":\"frobenius\",\"alpha_W\":0.0,\"alpha_H\":0.0,\"l1_ratio\":0.0},\n",
    "                     {\"beta_loss\":\"kullback-leibler\",\"solver\":\"mu\",\"alpha_W\":0.0,\"alpha_H\":0.0,\"l1_ratio\":0.0}]\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # Read CSV with tab delimiter and handle potential parsing issues\n",
    "    try:\n",
    "        # Try with newer pandas syntax first\n",
    "        df = pd.read_csv(csv_path, sep='\\t', on_bad_lines='skip', quoting=3)  # quoting=3 for QUOTE_NONE\n",
    "    except TypeError:\n",
    "        # Fall back to older pandas syntax\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, sep='\\t', error_bad_lines=False, warn_bad_lines=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV: {e}\")\n",
    "            # Last resort: read line by line and parse manually\n",
    "            print(\"Reading file line by line...\")\n",
    "            import csv\n",
    "            data = []\n",
    "            with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f, delimiter='\\t')\n",
    "                header = next(reader)\n",
    "                for i, row in enumerate(reader):\n",
    "                    if len(row) >= 4:  # Ensure we have at least 4 columns\n",
    "                        data.append(row[:4])  # Take only first 4 columns\n",
    "                    if i > 10000:  # Limit for testing\n",
    "                        break\n",
    "            df = pd.DataFrame(data, columns=header[:4])\n",
    "    \n",
    "    print(f\"Loaded {len(df)} rows. Using text column: {text_col}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    # 1. Raw cleaning\n",
    "    print(\"Cleaning raw text...\")\n",
    "    df[\"clean_raw\"] = df[text_col].astype(str).apply(clean_text_raw)\n",
    "    docs = df[\"clean_raw\"].tolist()\n",
    "    if test_run_docs:\n",
    "        docs = docs[:test_run_docs]\n",
    "        print(f\"Test run with first {test_run_docs} documents\")\n",
    "\n",
    "    # 2. Lemmatize + pos filtering (spaCy)\n",
    "    print(\"Lemmatizing with spaCy (this can take a while)...\")\n",
    "    tokenized = spacy_lemmatize(docs)\n",
    "\n",
    "    # 3. Bigram/trigram\n",
    "    print(\"Building bigrams/trigrams...\")\n",
    "    texts_ngram, bigram_mod, trigram_mod = make_ngrams(tokenized, min_count=5, threshold=50)\n",
    "\n",
    "    # 4. Dictionary for coherence (gensim)\n",
    "    dictionary = Dictionary(texts_ngram)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    print(f\"Dictionary size after filtering: {len(dictionary)}\")\n",
    "\n",
    "    # Prepare texts for coherence (list of token lists)\n",
    "    texts_for_coherence = texts_ngram\n",
    "\n",
    "    # 5. Try variant TF-IDF max_features and NMF hyperparams -> evaluate coherence\n",
    "    best = {\"coherence\": -999, \"params\": None, \"model\": None, \"vectorizer\": None, \"H\": None, \"W\": None, \"tfidf_matrix\": None}\n",
    "    results = []\n",
    "    for max_features in tfidf_max_features_options:\n",
    "        print(f\"\\n=== TF-IDF max_features = {max_features} ===\")\n",
    "        tfidf = TfidfVectorizer(\n",
    "            max_df=0.95, min_df=2, max_features=max_features,\n",
    "            stop_words=None, ngram_range=(1,2)\n",
    "        )\n",
    "        joined_texts = join_tokens(texts_ngram)\n",
    "        X = tfidf.fit_transform(joined_texts)\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        print(f\"TF-IDF shape: {X.shape}\")\n",
    "\n",
    "        for nmf_kw in nmf_kwargs_grid:\n",
    "            for n_topics in n_topic_range:\n",
    "                print(f\"Training NMF: topics={n_topics}, params={nmf_kw}\")\n",
    "                # create NMF with given params - filter valid parameters\n",
    "                valid_nmf_params = {}\n",
    "                for k, v in nmf_kw.items():\n",
    "                    if k in ['beta_loss', 'solver', 'alpha_W', 'alpha_H', 'l1_ratio']:\n",
    "                        valid_nmf_params[k] = v\n",
    "                \n",
    "                nmf = NMF(n_components=n_topics, random_state=42, init=\"nndsvda\", max_iter=400, **valid_nmf_params)\n",
    "                W = nmf.fit_transform(X)\n",
    "                H = nmf.components_\n",
    "                # Extract top words, build gensim topics format\n",
    "                top_words = top_words_from_components(H, feature_names, topn=12)\n",
    "\n",
    "                # Build a gensim coherence model\n",
    "                cm = CoherenceModel(topics=top_words, texts=texts_for_coherence, dictionary=dictionary, coherence=\"c_v\")\n",
    "                coherence = cm.get_coherence()\n",
    "                print(f\"--> Coherence (c_v): {coherence:.4f}\")\n",
    "\n",
    "                results.append({\n",
    "                    \"max_features\": max_features,\n",
    "                    \"nmf_params\": nmf_kw,\n",
    "                    \"n_topics\": n_topics,\n",
    "                    \"coherence\": coherence,\n",
    "                    \"model\": nmf,\n",
    "                    \"W\": W,\n",
    "                    \"H\": H,\n",
    "                    \"tfidf\": tfidf\n",
    "                })\n",
    "\n",
    "                # Track best\n",
    "                if coherence > best[\"coherence\"]:\n",
    "                    best.update({\n",
    "                        \"coherence\": coherence,\n",
    "                        \"params\": {\"max_features\": max_features, \"nmf_params\": nmf_kw, \"n_topics\": n_topics},\n",
    "                        \"model\": nmf,\n",
    "                        \"H\": H,\n",
    "                        \"W\": W,\n",
    "                        \"tfidf_matrix\": X,\n",
    "                        \"vectorizer\": tfidf,\n",
    "                        \"feature_names\": feature_names\n",
    "                    })\n",
    "\n",
    "    # 6. Summary of best model\n",
    "    print(\"\\n=== Best model summary ===\")\n",
    "    pprint(best[\"params\"])\n",
    "    print(f\"Best coherence: {best['coherence']:.4f}\")\n",
    "\n",
    "    # Save best artifacts\n",
    "    joblib.dump(best[\"model\"], os.path.join(out_dir, \"best_nmf_model.joblib\"))\n",
    "    joblib.dump(best[\"vectorizer\"], os.path.join(out_dir, \"best_tfidf_vectorizer.joblib\"))\n",
    "    with open(os.path.join(out_dir, \"best_params.json\"), \"w\") as f:\n",
    "        json.dump(best[\"params\"], f, indent=2)\n",
    "    print(f\"Saved best model & vectorizer to {out_dir}\")\n",
    "\n",
    "    # 7. Plot coherence vs topics (from results)\n",
    "    df_res = pd.DataFrame([{\"n_topics\": r[\"n_topics\"], \"coherence\": r[\"coherence\"], \"max_features\": r[\"max_features\"], \"beta_loss\": r[\"nmf_params\"].get(\"beta_loss\", \"\")} for r in results])\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for mf in df_res[\"max_features\"].unique():\n",
    "        subset = df_res[df_res[\"max_features\"]==mf]\n",
    "        subset = subset.sort_values(\"n_topics\")\n",
    "        plt.plot(subset[\"n_topics\"], subset[\"coherence\"], marker=\"o\", label=f\"max_feat={mf}\")\n",
    "    plt.xlabel(\"Number of topics\")\n",
    "    plt.ylabel(\"Coherence (c_v)\")\n",
    "    plt.title(\"NMF Coherence (c_v) vs # topics\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"coherence_vs_topics.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    # 8. Print & plot top words and wordclouds for best model\n",
    "    best_H = best[\"H\"]\n",
    "    best_feature_names = best[\"feature_names\"]\n",
    "    best_n = best[\"params\"][\"n_topics\"]\n",
    "    topics = top_words_from_components(best_H, best_feature_names, topn=15)\n",
    "    print(\"\\nTop words per topic (best model):\")\n",
    "    for i, twords in enumerate(topics, 1):\n",
    "        print(f\"Topic {i}: {', '.join(twords)}\")\n",
    "        plot_wordcloud(twords, title=f\"Topic {i}\", savepath=os.path.join(out_dir, f\"topic_{i}_wordcloud.png\"))\n",
    "\n",
    "    # 9. Assign dominant topic to each document & save\n",
    "    print(\"\\nAssigning dominant topic to documents...\")\n",
    "    W_best = best[\"W\"]\n",
    "    dominant = np.argmax(W_best, axis=1)\n",
    "    # If we used test_run_docs, only a subset was processed. Align lengths.\n",
    "    df_out = df.copy().iloc[:len(dominant)].reset_index(drop=True)\n",
    "    df_out[\"dominant_topic\"] = dominant\n",
    "    df_out.to_csv(os.path.join(out_dir, \"documents_with_topics.csv\"), index=False)\n",
    "    print(f\"Saved document-topic assignments to {out_dir}/documents_with_topics.csv\")\n",
    "\n",
    "    return best, results, os.path.join(out_dir, \"documents_with_topics.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# Run the pipeline directly in notebook\n",
    "# ---------------------------\n",
    "# Run the NMF pipeline with default parameters\n",
    "# You can modify these parameters as needed\n",
    "csv_path = r\"G:\\Topic Modeling Project\\artifacts\\bbc-news-data.csv\"\n",
    "output_dir = \"nmf_results\"\n",
    "test_docs = None  # Set to a small number like 100 for quick testing\n",
    "\n",
    "print(\"Starting NMF topic modeling pipeline...\")\n",
    "best_model, results, output_file = run_nmf_pipeline(\n",
    "    csv_path=csv_path,\n",
    "    out_dir=output_dir,\n",
    "    test_run_docs=test_docs\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline completed successfully!\")\n",
    "print(f\"Results saved to: {output_dir}\")\n",
    "print(f\"Document-topic assignments saved to: {output_file}\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
