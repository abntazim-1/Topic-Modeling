{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Topic Modeling Project","text":"<p>Welcome to the Topic Modeling Project! This project provides a comprehensive toolkit for automated topic modeling using Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) algorithms.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multiple Topic Modeling Algorithms: Support for LDA and NMF models</li> <li>Automated Preprocessing: Text cleaning, tokenization, and vectorization</li> <li>Model Evaluation: Comprehensive evaluation metrics including coherence and perplexity</li> <li>Easy-to-use API: Simple Python interface for all operations</li> <li>Production Ready: Robust error handling and logging</li> <li>Extensible: Modular design for easy customization</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from src.topics.lda_model import LDAModeler\nfrom src.data.data_preprocessing import TextPreprocessor\n\n# Initialize preprocessor\npreprocessor = TextPreprocessor()\ntexts = preprocessor.preprocess_texts(raw_texts)\n\n# Initialize LDA model\nlda = LDAModeler(num_topics=10)\nlda.train(corpus, dictionary)\n\n# Get topics\ntopics = lda.get_topics()\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>See the Installation Guide for detailed setup instructions.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and setup</li> <li>User Guide - How to use the project</li> <li>API Reference - Complete API documentation</li> <li>Examples - Code examples and tutorials</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to the Topic Modeling Project! This document provides guidelines for contributing.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository</li> <li>Clone your fork locally</li> <li>Create a new branch for your feature</li> <li>Make your changes</li> <li>Add tests for new functionality</li> <li>Ensure all tests pass</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone your fork\ngit clone https://github.com/yourusername/topic-modeling-project.git\ncd topic-modeling-project\n\n# Install development dependencies\npip install -r requirements-dev.txt\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use the following tools for code quality:</p> <ul> <li>Black: Code formatting</li> <li>Flake8: Linting</li> <li>MyPy: Type checking</li> <li>Pytest: Testing</li> </ul> <p>Run these before submitting:</p> <pre><code>black src/\nflake8 src/\nmypy src/\npytest tests/\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>When adding new features:</p> <ol> <li>Update docstrings using Google style</li> <li>Add examples to the documentation</li> <li>Update the API reference if needed</li> <li>Test that <code>mkdocs serve</code> works correctly</li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your code follows the style guidelines</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> <li>Submit a clear description of your changes</li> <li>Link any related issues</li> </ol> <p>Thank you for contributing!</p>"},{"location":"examples/","title":"Examples","text":"<p>This page contains practical examples of using the Topic Modeling Project.</p>"},{"location":"examples/#basic-topic-modeling","title":"Basic Topic Modeling","text":"<pre><code>from src.topics.lda_model import LDAModeler\nfrom src.data.data_preprocessing import TextPreprocessor\nfrom src.features.tfidf import TFIDFVectorizer\n\n# Sample data\ntexts = [\n    \"Machine learning is a subset of artificial intelligence.\",\n    \"Deep learning uses neural networks with multiple layers.\",\n    \"Natural language processing helps computers understand text.\",\n    \"Computer vision enables machines to interpret visual information.\",\n    \"Data science combines statistics, programming, and domain expertise.\"\n]\n\n# Preprocess texts\npreprocessor = TextPreprocessor()\nprocessed_texts = preprocessor.preprocess_texts(texts)\n\n# Vectorize\nvectorizer = TFIDFVectorizer()\ncorpus, dictionary = vectorizer.fit_transform(processed_texts)\n\n# Train model\nlda = LDAModeler(num_topics=3)\nlda.train(corpus, dictionary)\n\n# Get topics\ntopics = lda.get_topics()\nfor topic in topics:\n    print(f\"Topic {topic['topic_id']}: {topic['words']}\")\n</code></pre>"},{"location":"examples/#model-comparison","title":"Model Comparison","text":"<pre><code>from src.topics.lda_model import LDAModeler\nfrom src.topics.nmf_model import NMFModeler\n\n# Train both models\nlda = LDAModeler(num_topics=5)\nnmf = NMFModeler(num_topics=5)\n\nlda.train(corpus, dictionary)\nnmf.train(corpus, dictionary)\n\n# Compare coherence scores\nlda_coherence = lda.compute_coherence_score(processed_texts, dictionary)\nnmf_coherence = nmf.compute_coherence_score(processed_texts, dictionary)\n\nprint(f\"LDA Coherence: {lda_coherence:.4f}\")\nprint(f\"NMF Coherence: {nmf_coherence:.4f}\")\n</code></pre>"},{"location":"examples/#document-classification","title":"Document Classification","text":"<pre><code># Get dominant topic for each document\ndominant_topics = lda.get_dominant_topic_per_doc(corpus)\n\nfor i, topic_id in enumerate(dominant_topics):\n    print(f\"Document {i}: Topic {topic_id}\")\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>This section contains the complete API documentation for the Topic Modeling Project, automatically generated from Python docstrings.</p>"},{"location":"api/#modules","title":"Modules","text":""},{"location":"api/#src.topics.lda_model","title":"src.topics.lda_model","text":""},{"location":"api/#src.topics.lda_model-classes","title":"Classes","text":""},{"location":"api/#src.topics.lda_model.LDAModeler","title":"src.topics.lda_model.LDAModeler","text":"<pre><code>LDAModeler(num_topics: int = 10, passes: int = 10, chunksize: int = 100, alpha: str = 'auto', random_state: int = 42)\n</code></pre> <p>LDA Topic Modeling wrapper using gensim.models.LdaModel. Provides training, evaluation, topic extraction, and persistence utilities with robust logging.</p> <p>Initializes the LDA Modeler. Args:     num_topics: Number of topics for LDA.     passes: Number of passes over corpus during training.     chunksize: Number of documents to use in each training chunk.     alpha: Document-topic density prior.     random_state: Random state for reproducibility.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def __init__(\n    self,\n    num_topics: int = 10,\n    passes: int = 10,\n    chunksize: int = 100,\n    alpha: str = 'auto',\n    random_state: int = 42\n) -&gt; None:\n    \"\"\"\n    Initializes the LDA Modeler.\n    Args:\n        num_topics: Number of topics for LDA.\n        passes: Number of passes over corpus during training.\n        chunksize: Number of documents to use in each training chunk.\n        alpha: Document-topic density prior.\n        random_state: Random state for reproducibility.\n    \"\"\"\n    self.num_topics = num_topics\n    self.passes = passes\n    self.chunksize = chunksize\n    self.alpha = alpha\n    self.random_state = random_state\n    self.model: Optional[LdaModel] = None\n    self.logger = get_logger(__name__)\n    self.logger.info(f\"LDAModeler initialized with num_topics={num_topics}, passes={passes}, chunksize={chunksize}, alpha={alpha}.\")\n</code></pre>"},{"location":"api/#src.topics.lda_model.LDAModeler-functions","title":"Functions","text":""},{"location":"api/#src.topics.lda_model.LDAModeler.train","title":"src.topics.lda_model.LDAModeler.train","text":"<pre><code>train(corpus: List[List[tuple]], id2word: Dictionary) -&gt; None\n</code></pre> <p>Fits the LDA model to the provided corpus. Args:     corpus: Gensim corpus (list of bag-of-words).     id2word: Gensim dictionary object. Raises:     DataValidationError: On invalid corpus/dictionary.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def train(self, corpus: List[List[tuple]], id2word: Dictionary) -&gt; None:\n    \"\"\"\n    Fits the LDA model to the provided corpus.\n    Args:\n        corpus: Gensim corpus (list of bag-of-words).\n        id2word: Gensim dictionary object.\n    Raises:\n        DataValidationError: On invalid corpus/dictionary.\n    \"\"\"\n    try:\n        if not corpus or not isinstance(corpus, list):\n            raise DataValidationError(\"Invalid or empty corpus provided.\")\n        if not isinstance(id2word, Dictionary):\n            raise DataValidationError(\"id2word must be a gensim Dictionary object.\")\n        self.logger.info(\"Starting LDA model training...\")\n        self.model = LdaModel(\n            corpus=corpus,\n            id2word=id2word,\n            num_topics=self.num_topics,\n            passes=self.passes,\n            chunksize=self.chunksize,\n            alpha=self.alpha,\n            random_state=self.random_state\n        )\n        self.logger.info(f\"LDA model training complete. Number of topics: {self.model.num_topics}\")\n    except Exception as e:\n        self.logger.error(f\"Error during LDA model training: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.topics.lda_model.LDAModeler.get_topics","title":"src.topics.lda_model.LDAModeler.get_topics","text":"<pre><code>get_topics(num_words: int = 10) -&gt; List[Dict[str, Union[int, List[Tuple[str, float]]]]]\n</code></pre> <p>Returns the top words for each topic. Args:     num_words: Number of top words per topic. Returns:     List of topics, each as dict of topic_id and (word, weight) tuples. Raises:     AppException: If model is not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def get_topics(self, num_words: int = 10) -&gt; List[Dict[str, Union[int, List[Tuple[str, float]]]]]:\n    \"\"\"\n    Returns the top words for each topic.\n    Args:\n        num_words: Number of top words per topic.\n    Returns:\n        List of topics, each as dict of topic_id and (word, weight) tuples.\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained yet.\")\n        topics = []\n        for idx, topic in self.model.show_topics(num_topics=self.num_topics, num_words=num_words, formatted=False):\n            topics.append({\n                \"topic_id\": idx,\n                \"words\": [(word, round(weight, 4)) for word, weight in topic]\n            })\n        self.logger.info(f\"Extracted top {num_words} words for each topic.\")\n        return topics\n    except Exception as e:\n        self.logger.error(f\"Failed to get topic words: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.topics.lda_model.LDAModeler.compute_coherence_score","title":"src.topics.lda_model.LDAModeler.compute_coherence_score","text":"<pre><code>compute_coherence_score(texts: List[List[str]], id2word: Dictionary, coherence: str = 'c_v') -&gt; float\n</code></pre> <p>Compute the coherence score for trained topics. Args:     texts: Tokenized texts for coherence computation.     id2word: Gensim dictionary object.     coherence: Coherence type (default: 'c_v'). Returns:     Coherence score (float). Raises:     AppException: On error or if model not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def compute_coherence_score(self, texts: List[List[str]], id2word: Dictionary, coherence: str = 'c_v') -&gt; float:\n    \"\"\"\n    Compute the coherence score for trained topics.\n    Args:\n        texts: Tokenized texts for coherence computation.\n        id2word: Gensim dictionary object.\n        coherence: Coherence type (default: 'c_v').\n    Returns:\n        Coherence score (float).\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained.\")\n        coherence_model = CoherenceModel(\n            model=self.model,\n            texts=texts,\n            dictionary=id2word,\n            coherence=coherence\n        )\n        score = coherence_model.get_coherence()\n        self.logger.info(f\"Coherence ({coherence}) score: {score:.4f}\")\n        return score\n    except Exception as e:\n        self.logger.error(f\"Failed to compute coherence score: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.topics.lda_model.LDAModeler.compute_perplexity","title":"src.topics.lda_model.LDAModeler.compute_perplexity","text":"<pre><code>compute_perplexity(corpus: List[List[tuple]]) -&gt; float\n</code></pre> <p>Compute model perplexity on the given corpus. Args:     corpus: Corpus to compute perplexity against. Returns:     Perplexity value (float). Raises:     AppException: On error or if model not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def compute_perplexity(self, corpus: List[List[tuple]]) -&gt; float:\n    \"\"\"\n    Compute model perplexity on the given corpus.\n    Args:\n        corpus: Corpus to compute perplexity against.\n    Returns:\n        Perplexity value (float).\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained.\")\n        perplexity = self.model.log_perplexity(corpus)\n        self.logger.info(f\"Model log perplexity: {perplexity:.4f}\")\n        return perplexity\n    except Exception as e:\n        self.logger.error(f\"Failed to compute perplexity: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.topics.lda_model.LDAModeler.get_dominant_topic_per_doc","title":"src.topics.lda_model.LDAModeler.get_dominant_topic_per_doc","text":"<pre><code>get_dominant_topic_per_doc(corpus: List[List[tuple]]) -&gt; List[int]\n</code></pre> <p>Returns the most likely topic for each document in the corpus. Args:     corpus: Corpus in BOW format. Returns:     List of dominant topic indices per document. Raises:     AppException: On error or if model not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def get_dominant_topic_per_doc(self, corpus: List[List[tuple]]) -&gt; List[int]:\n    \"\"\"\n    Returns the most likely topic for each document in the corpus.\n    Args:\n        corpus: Corpus in BOW format.\n    Returns:\n        List of dominant topic indices per document.\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained.\")\n        dominant_topics = []\n        for doc_bow in corpus:\n            topic_probs = self.model.get_document_topics(doc_bow)\n            if topic_probs:\n                dominant_topic = max(topic_probs, key=lambda tup: tup[1])[0]\n            else:\n                dominant_topic = -1\n            dominant_topics.append(dominant_topic)\n        self.logger.info(\"Extracted dominant topic for each document.\")\n        return dominant_topics\n    except Exception as e:\n        self.logger.error(f\"Failed to extract dominant topics: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.topics.lda_model.LDAModeler.save_model","title":"src.topics.lda_model.LDAModeler.save_model","text":"<pre><code>save_model(path: str) -&gt; None\n</code></pre> <p>Saves the trained model to a file. Args:     path: Path to save model. Raises:     AppException: On error or if model not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def save_model(self, path: str) -&gt; None:\n    \"\"\"\n    Saves the trained model to a file.\n    Args:\n        path: Path to save model.\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained and cannot be saved.\")\n        self.model.save(path)\n        self.logger.info(f\"LDA model saved to: {path}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to save LDA model: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.topics.lda_model.LDAModeler.load_model","title":"src.topics.lda_model.LDAModeler.load_model","text":"<pre><code>load_model(path: str) -&gt; None\n</code></pre> <p>Loads a model from the given path. Args:     path: Path to load model from. Raises:     AppException: On error loading the model.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def load_model(self, path: str) -&gt; None:\n    \"\"\"\n    Loads a model from the given path.\n    Args:\n        path: Path to load model from.\n    Raises:\n        AppException: On error loading the model.\n    \"\"\"\n    try:\n        self.model = LdaModel.load(path)\n        self.logger.info(f\"LDA model loaded from: {path}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to load LDA model: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.topics.lda_model-functions","title":"Functions","text":""},{"location":"api/#src.topics.nmf_model","title":"src.topics.nmf_model","text":""},{"location":"api/#src.topics.nmf_model-classes","title":"Classes","text":""},{"location":"api/#src.topics.nmf_model.NMFModeler","title":"src.topics.nmf_model.NMFModeler","text":"<pre><code>NMFModeler(num_topics: int = 10, init: str = 'nndsvda', max_iter: int = 500, random_state: int = 42, alpha: float = 0.1, l1_ratio: float = 0.5)\n</code></pre> <p>Non-Negative Matrix Factorization (NMF) Topic Modeling wrapper using  sklearn.decomposition.NMF. Provides training, evaluation, topic extraction,  visualization, and persistence utilities with robust logging and error handling.</p> <p>Initializes the NMF Modeler.</p> <p>Parameters:</p> Name Type Description Default <code>num_topics</code> <code>int</code> <p>Number of topics to extract.</p> <code>10</code> <code>init</code> <code>str</code> <p>Initialization method ('random', 'nndsvd', 'nndsvda', 'nndsvdar').</p> <code>'nndsvda'</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>500</code> <code>random_state</code> <code>int</code> <p>Random state for reproducibility.</p> <code>42</code> <code>alpha</code> <code>float</code> <p>Regularization parameter (multiplier for regularization terms).</p> <code>0.1</code> <code>l1_ratio</code> <code>float</code> <p>Ratio of L1 to L2 regularization (0.0 = L2 only, 1.0 = L1 only).</p> <code>0.5</code> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def __init__(\n    self,\n    num_topics: int = 10,\n    init: str = 'nndsvda',\n    max_iter: int = 500,\n    random_state: int = 42,\n    alpha: float = 0.1,\n    l1_ratio: float = 0.5\n) -&gt; None:\n    \"\"\"\n    Initializes the NMF Modeler.\n\n    Args:\n        num_topics: Number of topics to extract.\n        init: Initialization method ('random', 'nndsvd', 'nndsvda', 'nndsvdar').\n        max_iter: Maximum number of iterations.\n        random_state: Random state for reproducibility.\n        alpha: Regularization parameter (multiplier for regularization terms).\n        l1_ratio: Ratio of L1 to L2 regularization (0.0 = L2 only, 1.0 = L1 only).\n    \"\"\"\n    self.num_topics = num_topics\n    self.init = init\n    self.max_iter = max_iter\n    self.random_state = random_state\n    self.alpha = alpha\n    self.l1_ratio = l1_ratio\n\n    self.model: Optional[NMF] = None\n    self.feature_names: Optional[List[str]] = None\n    self.document_topic_matrix: Optional[np.ndarray] = None\n    self.topic_term_matrix: Optional[np.ndarray] = None\n\n    self.logger = get_logger(__name__)\n    self.logger.info(\n        f\"NMFModeler initialized with num_topics={num_topics}, init={init}, \"\n        f\"max_iter={max_iter}, alpha={alpha}, l1_ratio={l1_ratio}\"\n    )\n</code></pre>"},{"location":"api/#src.topics.nmf_model.NMFModeler-functions","title":"Functions","text":""},{"location":"api/#src.topics.nmf_model.NMFModeler.train","title":"src.topics.nmf_model.NMFModeler.train","text":"<pre><code>train(tfidf_matrix: Union[ndarray, csr_matrix], feature_names: List[str]) -&gt; None\n</code></pre> <p>Fits the NMF model to the provided TF-IDF matrix.</p> <p>Parameters:</p> Name Type Description Default <code>tfidf_matrix</code> <code>Union[ndarray, csr_matrix]</code> <p>TF-IDF matrix (can be sparse or dense).</p> required <code>feature_names</code> <code>List[str]</code> <p>List of feature names (vocabulary).</p> required <p>Raises:</p> Type Description <code>DataValidationError</code> <p>On invalid input data.</p> <code>AppException</code> <p>On training errors.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def train(\n    self, \n    tfidf_matrix: Union[np.ndarray, csr_matrix], \n    feature_names: List[str]\n) -&gt; None:\n    \"\"\"\n    Fits the NMF model to the provided TF-IDF matrix.\n\n    Args:\n        tfidf_matrix: TF-IDF matrix (can be sparse or dense).\n        feature_names: List of feature names (vocabulary).\n\n    Raises:\n        DataValidationError: On invalid input data.\n        AppException: On training errors.\n    \"\"\"\n    try:\n        # Validate inputs\n        if tfidf_matrix is None or (isinstance(tfidf_matrix, np.ndarray) and tfidf_matrix.size == 0):\n            raise DataValidationError(\"TF-IDF matrix is empty or None.\")\n\n        if not feature_names or not isinstance(feature_names, list):\n            raise DataValidationError(\"feature_names must be a non-empty list.\")\n\n        # Convert to sparse matrix if dense\n        if isinstance(tfidf_matrix, np.ndarray):\n            self.logger.info(\"Converting dense matrix to sparse format for efficiency.\")\n            tfidf_matrix = csr_matrix(tfidf_matrix)\n\n        # Validate matrix dimensions\n        if tfidf_matrix.shape[1] != len(feature_names):\n            raise DataValidationError(\n                f\"Matrix columns ({tfidf_matrix.shape[1]}) must match \"\n                f\"feature_names length ({len(feature_names)}).\"\n            )\n\n        self.feature_names = feature_names\n\n        self.logger.info(\n            f\"Starting NMF training on matrix of shape {tfidf_matrix.shape}...\"\n        )\n\n        # Initialize and train NMF model\n        self.model = NMF(\n            n_components=self.num_topics,\n            init=self.init,\n            max_iter=self.max_iter,\n            random_state=self.random_state,\n            alpha_W=self.alpha,  # Changed from alpha to alpha_W\n            l1_ratio=self.l1_ratio,\n            verbose=0\n        )\n\n        # Fit and transform\n        self.document_topic_matrix = self.model.fit_transform(tfidf_matrix)\n        self.topic_term_matrix = self.model.components_\n\n        self.logger.info(\n            f\"NMF training complete. Reconstruction error: \"\n            f\"{self.model.reconstruction_err_:.4f}\"\n        )\n        self.logger.info(\n            f\"Document-topic matrix shape: {self.document_topic_matrix.shape}\"\n        )\n        self.logger.info(\n            f\"Topic-term matrix shape: {self.topic_term_matrix.shape}\"\n        )\n\n    except DataValidationError as e:\n        self.logger.error(f\"Data validation error during NMF training: {e}\")\n        raise\n    except Exception as e:\n        self.logger.error(f\"Error during NMF model training: {e}\")\n        raise AppException(f\"NMF training failed: {str(e)}\")\n</code></pre>"},{"location":"api/#src.topics.nmf_model.NMFModeler.get_topics","title":"src.topics.nmf_model.NMFModeler.get_topics","text":"<pre><code>get_topics(num_words: int = 10) -&gt; List[Dict[str, Union[int, List[Tuple[str, float]]]]]\n</code></pre> <p>Returns the top words for each topic.</p> <p>Parameters:</p> Name Type Description Default <code>num_words</code> <code>int</code> <p>Number of top words per topic.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Union[int, List[Tuple[str, float]]]]]</code> <p>List of topics, each as dict with topic_id and (word, weight) tuples.</p> <p>Raises:</p> Type Description <code>AppException</code> <p>If model is not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def get_topics(\n    self, \n    num_words: int = 10\n) -&gt; List[Dict[str, Union[int, List[Tuple[str, float]]]]]:\n    \"\"\"\n    Returns the top words for each topic.\n\n    Args:\n        num_words: Number of top words per topic.\n\n    Returns:\n        List of topics, each as dict with topic_id and (word, weight) tuples.\n\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.model is None or self.topic_term_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        if self.feature_names is None:\n            raise AppException(\"Feature names not available.\")\n\n        topics = []\n\n        for topic_idx in range(self.num_topics):\n            # Get top word indices for this topic\n            top_indices = self.topic_term_matrix[topic_idx].argsort()[-num_words:][::-1]\n            top_words = [\n                (self.feature_names[i], float(self.topic_term_matrix[topic_idx][i]))\n                for i in top_indices\n            ]\n\n            topics.append({\n                \"topic_id\": topic_idx,\n                \"words\": [(word, round(weight, 4)) for word, weight in top_words]\n            })\n\n            # Log top words\n            words_str = \", \".join([f\"{word}({weight:.3f})\" for word, weight in top_words[:5]])\n            self.logger.info(f\"Topic {topic_idx}: {words_str}...\")\n\n        self.logger.info(f\"Extracted top {num_words} words for {self.num_topics} topics.\")\n        return topics\n\n    except AppException as e:\n        self.logger.error(f\"Failed to get topic words: {e}\")\n        raise\n    except Exception as e:\n        self.logger.error(f\"Unexpected error in get_topics: {e}\")\n        raise AppException(f\"Failed to extract topics: {str(e)}\")\n</code></pre>"},{"location":"api/#src.topics.nmf_model.NMFModeler.get_document_topics","title":"src.topics.nmf_model.NMFModeler.get_document_topics","text":"<pre><code>get_document_topics() -&gt; List[Dict[str, Union[int, float]]]\n</code></pre> <p>Returns the dominant topic for each document along with topic distribution.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Union[int, float]]]</code> <p>List of dicts containing dominant_topic, probability, and full distribution.</p> <p>Raises:</p> Type Description <code>AppException</code> <p>If model is not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def get_document_topics(self) -&gt; List[Dict[str, Union[int, float]]]:\n    \"\"\"\n    Returns the dominant topic for each document along with topic distribution.\n\n    Returns:\n        List of dicts containing dominant_topic, probability, and full distribution.\n\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.document_topic_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        document_topics = []\n\n        for doc_idx, topic_dist in enumerate(self.document_topic_matrix):\n            # Normalize to get probabilities\n            topic_probs = topic_dist / (topic_dist.sum() + 1e-10)\n            dominant_topic = int(np.argmax(topic_probs))\n            dominant_prob = float(topic_probs[dominant_topic])\n\n            document_topics.append({\n                \"document_id\": doc_idx,\n                \"dominant_topic\": dominant_topic,\n                \"dominant_probability\": round(dominant_prob, 4),\n                \"topic_distribution\": [round(float(p), 4) for p in topic_probs]\n            })\n\n        self.logger.info(\n            f\"Extracted dominant topics for {len(document_topics)} documents.\"\n        )\n        return document_topics\n\n    except Exception as e:\n        self.logger.error(f\"Failed to extract document topics: {e}\")\n        raise AppException(f\"Document topic extraction failed: {str(e)}\")\n</code></pre>"},{"location":"api/#src.topics.nmf_model.NMFModeler.compute_coherence","title":"src.topics.nmf_model.NMFModeler.compute_coherence","text":"<pre><code>compute_coherence(texts: List[List[str]], measure: str = 'c_v', num_words: int = 10) -&gt; float\n</code></pre> <p>Compute coherence score for the extracted topics using Gensim.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[List[str]]</code> <p>List of tokenized documents (list of list of strings).</p> required <code>measure</code> <code>str</code> <p>Coherence measure ('c_v', 'u_mass', 'c_uci', 'c_npmi').</p> <code>'c_v'</code> <code>num_words</code> <code>int</code> <p>Number of top words per topic to use for coherence.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>Coherence score (float).</p> <p>Raises:</p> Type Description <code>AppException</code> <p>On error or if model not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def compute_coherence(\n    self,\n    texts: List[List[str]],\n    measure: str = 'c_v',\n    num_words: int = 10\n) -&gt; float:\n    \"\"\"\n    Compute coherence score for the extracted topics using Gensim.\n\n    Args:\n        texts: List of tokenized documents (list of list of strings).\n        measure: Coherence measure ('c_v', 'u_mass', 'c_uci', 'c_npmi').\n        num_words: Number of top words per topic to use for coherence.\n\n    Returns:\n        Coherence score (float).\n\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None or self.topic_term_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        if not texts or not isinstance(texts, list):\n            raise DataValidationError(\"texts must be a non-empty list of tokenized documents.\")\n\n        # Create Gensim dictionary from texts\n        dictionary = Dictionary(texts)\n\n        # Extract top words for each topic\n        topic_words = []\n        for topic_idx in range(self.num_topics):\n            top_indices = self.topic_term_matrix[topic_idx].argsort()[-num_words:][::-1]\n            top_words = [self.feature_names[i] for i in top_indices]\n            topic_words.append(top_words)\n\n        # Compute coherence using Gensim\n        coherence_model = CoherenceModel(\n            topics=topic_words,\n            texts=texts,\n            dictionary=dictionary,\n            coherence=measure\n        )\n\n        coherence_score = coherence_model.get_coherence()\n\n        self.logger.info(\n            f\"Coherence score ({measure}): {coherence_score:.4f}\"\n        )\n\n        return coherence_score\n\n    except DataValidationError as e:\n        self.logger.error(f\"Data validation error in coherence computation: {e}\")\n        raise\n    except Exception as e:\n        self.logger.error(f\"Failed to compute coherence score: {e}\")\n        raise AppException(f\"Coherence computation failed: {str(e)}\")\n</code></pre>"},{"location":"api/#src.topics.nmf_model.NMFModeler.reduce_dimensions_pca","title":"src.topics.nmf_model.NMFModeler.reduce_dimensions_pca","text":"<pre><code>reduce_dimensions_pca(n_components: int = 2) -&gt; np.ndarray\n</code></pre> <p>Apply PCA to document-topic matrix for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of principal components (default: 2 for 2D viz).</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Reduced dimensional representation of documents.</p> <p>Raises:</p> Type Description <code>AppException</code> <p>If model is not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def reduce_dimensions_pca(\n    self,\n    n_components: int = 2\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply PCA to document-topic matrix for visualization.\n\n    Args:\n        n_components: Number of principal components (default: 2 for 2D viz).\n\n    Returns:\n        Reduced dimensional representation of documents.\n\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.document_topic_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        self.logger.info(\n            f\"Applying PCA to reduce dimensions to {n_components} components...\"\n        )\n\n        pca = PCA(n_components=n_components, random_state=self.random_state)\n        reduced_data = pca.fit_transform(self.document_topic_matrix)\n\n        explained_variance = pca.explained_variance_ratio_\n        self.logger.info(\n            f\"PCA complete. Explained variance: \"\n            f\"{', '.join([f'{v:.2%}' for v in explained_variance])}\"\n        )\n\n        return reduced_data\n\n    except Exception as e:\n        self.logger.error(f\"PCA dimensionality reduction failed: {e}\")\n        raise AppException(f\"PCA failed: {str(e)}\")\n</code></pre>"},{"location":"api/#src.topics.nmf_model.NMFModeler.visualize_topic_heatmap","title":"src.topics.nmf_model.NMFModeler.visualize_topic_heatmap","text":"<pre><code>visualize_topic_heatmap(num_words: int = 10, figsize: Tuple[int, int] = (12, 8), save_path: Optional[str] = None) -&gt; None\n</code></pre> <p>Visualize topic-term matrix as a heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>num_words</code> <code>int</code> <p>Number of top words per topic to display.</p> <code>10</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size (width, height).</p> <code>(12, 8)</code> <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the figure.</p> <code>None</code> <p>Raises:</p> Type Description <code>AppException</code> <p>If model is not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def visualize_topic_heatmap(\n    self,\n    num_words: int = 10,\n    figsize: Tuple[int, int] = (12, 8),\n    save_path: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Visualize topic-term matrix as a heatmap.\n\n    Args:\n        num_words: Number of top words per topic to display.\n        figsize: Figure size (width, height).\n        save_path: Optional path to save the figure.\n\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.model is None or self.topic_term_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        if self.feature_names is None:\n            raise AppException(\"Feature names not available.\")\n\n        self.logger.info(\"Creating topic-term heatmap visualization...\")\n\n        # Get top words for each topic\n        top_words_per_topic = []\n        heatmap_data = []\n\n        for topic_idx in range(self.num_topics):\n            top_indices = self.topic_term_matrix[topic_idx].argsort()[-num_words:][::-1]\n            top_words = [self.feature_names[i] for i in top_indices]\n            top_words_per_topic.extend(top_words)\n\n            # Extract weights for these words\n            weights = [self.topic_term_matrix[topic_idx][i] for i in top_indices]\n            heatmap_data.append(weights)\n\n        # Remove duplicates while preserving order\n        unique_words = []\n        seen = set()\n        for word in top_words_per_topic:\n            if word not in seen:\n                unique_words.append(word)\n                seen.add(word)\n\n        # Create heatmap data matrix\n        heatmap_matrix = np.zeros((self.num_topics, len(unique_words)))\n        word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n\n        for topic_idx in range(self.num_topics):\n            top_indices = self.topic_term_matrix[topic_idx].argsort()[-num_words:][::-1]\n            for word_idx in top_indices:\n                word = self.feature_names[word_idx]\n                if word in word_to_idx:\n                    heatmap_matrix[topic_idx, word_to_idx[word]] = \\\n                        self.topic_term_matrix[topic_idx][word_idx]\n\n        # Create heatmap\n        plt.figure(figsize=figsize)\n        sns.heatmap(\n            heatmap_matrix,\n            xticklabels=unique_words,\n            yticklabels=[f\"Topic {i}\" for i in range(self.num_topics)],\n            cmap=\"YlOrRd\",\n            cbar_kws={'label': 'Weight'},\n            linewidths=0.5\n        )\n\n        plt.title(\"Topic-Term Heatmap\", fontsize=16, fontweight='bold')\n        plt.xlabel(\"Top Terms\", fontsize=12)\n        plt.ylabel(\"Topics\", fontsize=12)\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            self.logger.info(f\"Heatmap saved to: {save_path}\")\n        else:\n            plt.show()\n\n        plt.close()\n\n    except Exception as e:\n        self.logger.error(f\"Heatmap visualization failed: {e}\")\n        raise AppException(f\"Visualization failed: {str(e)}\")\n</code></pre>"},{"location":"api/#src.topics.nmf_model.NMFModeler.save_model","title":"src.topics.nmf_model.NMFModeler.save_model","text":"<pre><code>save_model(path: str) -&gt; None\n</code></pre> <p>Saves the trained NMF model and associated data to a file using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the model.</p> required <p>Raises:</p> Type Description <code>AppException</code> <p>On error or if model not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def save_model(self, path: str) -&gt; None:\n    \"\"\"\n    Saves the trained NMF model and associated data to a file using joblib.\n\n    Args:\n        path: Path to save the model.\n\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"NMF model is not trained and cannot be saved.\")\n\n        model_data = {\n            'model': self.model,\n            'feature_names': self.feature_names,\n            'document_topic_matrix': self.document_topic_matrix,\n            'topic_term_matrix': self.topic_term_matrix,\n            'num_topics': self.num_topics,\n            'init': self.init,\n            'max_iter': self.max_iter,\n            'random_state': self.random_state,\n            'alpha': self.alpha,\n            'l1_ratio': self.l1_ratio\n        }\n\n        joblib.dump(model_data, path)\n        self.logger.info(f\"NMF model saved to: {path}\")\n\n    except Exception as e:\n        self.logger.error(f\"Failed to save NMF model: {e}\")\n        raise AppException(f\"Model save failed: {str(e)}\")\n</code></pre>"},{"location":"api/#src.topics.nmf_model.NMFModeler.load_model","title":"src.topics.nmf_model.NMFModeler.load_model","text":"<pre><code>load_model(path: str) -&gt; None\n</code></pre> <p>Loads an NMF model from the given path using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to load the model from.</p> required <p>Raises:</p> Type Description <code>AppException</code> <p>On error loading the model.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def load_model(self, path: str) -&gt; None:\n    \"\"\"\n    Loads an NMF model from the given path using joblib.\n\n    Args:\n        path: Path to load the model from.\n\n    Raises:\n        AppException: On error loading the model.\n    \"\"\"\n    try:\n        model_data = joblib.load(path)\n\n        self.model = model_data['model']\n        self.feature_names = model_data['feature_names']\n        self.document_topic_matrix = model_data['document_topic_matrix']\n        self.topic_term_matrix = model_data['topic_term_matrix']\n        self.num_topics = model_data['num_topics']\n        self.init = model_data['init']\n        self.max_iter = model_data['max_iter']\n        self.random_state = model_data['random_state']\n        self.alpha = model_data['alpha']\n        self.l1_ratio = model_data['l1_ratio']\n\n        self.logger.info(f\"NMF model loaded from: {path}\")\n        self.logger.info(f\"Model configuration: {self.num_topics} topics\")\n\n    except Exception as e:\n        self.logger.error(f\"Failed to load NMF model: {e}\")\n        raise AppException(f\"Model load failed: {str(e)}\")\n</code></pre>"},{"location":"api/#src.topics.nmf_model-functions","title":"Functions","text":""},{"location":"api/#src.data.data_preprocessing","title":"src.data.data_preprocessing","text":""},{"location":"api/#src.data.data_preprocessing-classes","title":"Classes","text":""},{"location":"api/#src.data.data_preprocessing.PreprocessingConfig","title":"src.data.data_preprocessing.PreprocessingConfig  <code>dataclass</code>","text":"<pre><code>PreprocessingConfig(text_column: str = 'content', stopwords: List[str] = (lambda: ['said', 'mr', 'say', 'also', 'would', 'one', 'two', 'us'])(), spacy_model: str = 'en_core_web_lg', output_dir: str = 'artifacts/', min_token_length: int = 2, gen_bigrams: bool = True, gen_trigrams: bool = True, logging_level: str = 'INFO', output_file: str = 'preprocessed_bbc_news.csv')\n</code></pre> <p>Configuration for text preprocessing.</p>"},{"location":"api/#src.data.data_preprocessing.TextCleaner","title":"src.data.data_preprocessing.TextCleaner","text":"<pre><code>TextCleaner(logger=None)\n</code></pre> <p>Text normalization utilities (lowercase, punctuation, HTML, contractions, etc.)</p> Source code in <code>src\\data\\data_preprocessing.py</code> <pre><code>def __init__(self, logger=None):\n    self.logger = logger or get_logger(__name__)\n    self.CONTRACTIONS = {\"it's\": \"it is\", \"can't\": \"cannot\", \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"}\n</code></pre>"},{"location":"api/#src.data.data_preprocessing.TextTokenizer","title":"src.data.data_preprocessing.TextTokenizer","text":"<pre><code>TextTokenizer(nlp, stopwords: set, min_token_length: int = 2, logger=None)\n</code></pre> <p>Tokenization, stopword removal, lemmatization, and n-gram handling.</p> Source code in <code>src\\data\\data_preprocessing.py</code> <pre><code>def __init__(self, nlp, stopwords: set, min_token_length: int = 2, logger=None):\n    self.nlp = nlp\n    self.stopwords = stopwords\n    self.min_token_length = min_token_length\n    self.logger = logger or get_logger(__name__)\n</code></pre>"},{"location":"api/#src.data.data_preprocessing.PreprocessingPipeline","title":"src.data.data_preprocessing.PreprocessingPipeline","text":"<pre><code>PreprocessingPipeline(config: PreprocessingConfig, logger=None)\n</code></pre> <p>Orchestrates full text preprocessing: cleaning, tokenization, n-grams. Accepts DataFrame or list.</p> Source code in <code>src\\data\\data_preprocessing.py</code> <pre><code>def __init__(self, config: PreprocessingConfig, logger=None):\n    self.config = config\n    self.logger = logger or get_logger(__name__)\n    try:\n        self.nlp = spacy.load(self.config.spacy_model, disable=[\"parser\", \"ner\"])\n    except OSError:\n        self.logger.error(f\"spaCy model not found: {self.config.spacy_model}. Please run: python -m spacy download {self.config.spacy_model}\")\n        raise AppException(f\"spaCy model not found: {self.config.spacy_model}\")\n    self.stopwords = merge_stopwords(self.config.stopwords, self.nlp)\n    self.cleaner = TextCleaner(self.logger)\n    self.tokenizer = TextTokenizer(self.nlp, self.stopwords, self.config.min_token_length, self.logger)\n</code></pre>"},{"location":"api/#src.data.data_preprocessing.TextVisualizer","title":"src.data.data_preprocessing.TextVisualizer","text":"<pre><code>TextVisualizer(logger=None)\n</code></pre> <p>Utility for word frequency, word cloud, and top-n tokens visualization (optional class, extensible for more viz later).</p> Source code in <code>src\\data\\data_preprocessing.py</code> <pre><code>def __init__(self, logger=None):\n    self.logger = logger or get_logger(__name__)\n    try:\n        import matplotlib.pyplot as plt\n        from wordcloud import WordCloud\n        self.plt = plt\n        self.WordCloud = WordCloud\n    except ImportError:\n        self.logger.warning(\"matplotlib or wordcloud not installed; TextVisualizer limited.\")\n        self.plt = None\n        self.WordCloud = None\n</code></pre>"},{"location":"api/#src.data.data_preprocessing-functions","title":"Functions","text":""},{"location":"api/#src.features.tfidf","title":"src.features.tfidf","text":""},{"location":"api/#src.features.tfidf-classes","title":"Classes","text":""},{"location":"api/#src.features.tfidf.TFIDFVectorizerWrapper","title":"src.features.tfidf.TFIDFVectorizerWrapper","text":"<pre><code>TFIDFVectorizerWrapper(ngram_range: Tuple[int, int] = (1, 2), min_df: int = 2, max_df: Union[int, float] = 0.95, max_features: Optional[int] = 10000, stop_words: Optional[List[str]] = None, logger: Optional[Logger] = None, **kwargs: Any)\n</code></pre> <p>Wrapper for scikit-learn's TfidfVectorizer with logging and configuration. Provides easy integration with text mining/topic modeling pipelines.</p> <p>Initialize the TF-IDF vectorizer wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>ngram_range</code> <code>Tuple[int, int]</code> <p>The lower and upper boundary of the n-grams (default (1,2)).</p> <code>(1, 2)</code> <code>min_df</code> <code>int</code> <p>Minimum document frequency for terms (default 2).</p> <code>2</code> <code>max_df</code> <code>Union[int, float]</code> <p>Max document frequency (int or proportion) (default 0.95).</p> <code>0.95</code> <code>max_features</code> <code>Optional[int]</code> <p>Max number of features (default 10000).</p> <code>10000</code> <code>stop_words</code> <code>Optional[List[str]]</code> <p>List of stopwords (optional, overrides TfidfVectorizer's built-ins).</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger instance.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Other parameters for scikit-learn TfidfVectorizer.</p> <code>{}</code> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def __init__(\n    self,\n    ngram_range: Tuple[int, int] = (1, 2),\n    min_df: int = 2,\n    max_df: Union[int, float] = 0.95,\n    max_features: Optional[int] = 10000,\n    stop_words: Optional[List[str]] = None,\n    logger: Optional[logging.Logger] = None,\n    **kwargs: Any\n):\n    \"\"\"\n    Initialize the TF-IDF vectorizer wrapper.\n\n    Args:\n        ngram_range: The lower and upper boundary of the n-grams (default (1,2)).\n        min_df: Minimum document frequency for terms (default 2).\n        max_df: Max document frequency (int or proportion) (default 0.95).\n        max_features: Max number of features (default 10000).\n        stop_words: List of stopwords (optional, overrides TfidfVectorizer's built-ins).\n        logger: Optional logger instance.\n        **kwargs: Other parameters for scikit-learn TfidfVectorizer.\n    \"\"\"\n    self.logger = logger or get_logger(__name__)\n    self.vectorizer = TfidfVectorizer(\n        ngram_range=ngram_range,\n        min_df=min_df,\n        max_df=max_df,\n        max_features=max_features,\n        stop_words=stop_words,\n        tokenizer=custom_tokenizer,\n        preprocessor=custom_preprocessor,\n        token_pattern=None,\n        **kwargs\n    )\n    self.fitted = False\n    self.logger.info(f\"Initialized TFIDFVectorizerWrapper with params: ngram_range={ngram_range}, min_df={min_df}, max_df={max_df}, max_features={max_features}\")\n</code></pre>"},{"location":"api/#src.features.tfidf.TFIDFVectorizerWrapper-functions","title":"Functions","text":""},{"location":"api/#src.features.tfidf.TFIDFVectorizerWrapper.fit","title":"src.features.tfidf.TFIDFVectorizerWrapper.fit","text":"<pre><code>fit(texts: List[Union[str, List[str]]]) -&gt; TFIDFVectorizerWrapper\n</code></pre> <p>Fit the vectorizer to a corpus of texts. Args:     texts: List of cleaned strings, or List of token lists. Returns:     self Raises:     AppException: If input is empty or invalid.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def fit(self, texts: List[Union[str, List[str]]]) -&gt; 'TFIDFVectorizerWrapper':\n    \"\"\"\n    Fit the vectorizer to a corpus of texts.\n    Args:\n        texts: List of cleaned strings, or List of token lists.\n    Returns:\n        self\n    Raises:\n        AppException: If input is empty or invalid.\n    \"\"\"\n    if not texts or not isinstance(texts, list):\n        self.logger.error(\"Input for fit is empty or not a list.\")\n        raise AppException(\"TFIDFVectorizerWrapper.fit: Input text list is empty or not a list.\")\n    self.vectorizer.fit(texts)\n    self.fitted = True\n    self.logger.info(f\"TF-IDF vectorizer fitted on {len(texts)} documents. Vocabulary size: {len(self.vectorizer.vocabulary_)}.\")\n    return self\n</code></pre>"},{"location":"api/#src.features.tfidf.TFIDFVectorizerWrapper.transform","title":"src.features.tfidf.TFIDFVectorizerWrapper.transform","text":"<pre><code>transform(texts: List[Union[str, List[str]]])\n</code></pre> <p>Transform new texts to the vectorized space. Args:     texts: List of cleaned strings, or List of token lists. Returns:     Sparse TF-IDF matrix Raises:     AppException: If vectorizer not fitted or input invalid.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def transform(self, texts: List[Union[str, List[str]]]):\n    \"\"\"\n    Transform new texts to the vectorized space.\n    Args:\n        texts: List of cleaned strings, or List of token lists.\n    Returns:\n        Sparse TF-IDF matrix\n    Raises:\n        AppException: If vectorizer not fitted or input invalid.\n    \"\"\"\n    if not texts or not isinstance(texts, list):\n        self.logger.error(\"Input for transform is empty or not a list.\")\n        raise AppException(\"TFIDFVectorizerWrapper.transform: Input text list is empty or not a list.\")\n    try:\n        matrix = self.vectorizer.transform(texts)\n        self.logger.info(f\"Transformed {len(texts)} documents. Resulting shape: {matrix.shape}.\")\n        return matrix\n    except NotFittedError as e:\n        self.logger.error(\"TF-IDF vectorizer not fitted.\")\n        raise AppException(\"TFIDFVectorizerWrapper: Vectorizer must be fitted before use.\") from e\n</code></pre>"},{"location":"api/#src.features.tfidf.TFIDFVectorizerWrapper.fit_transform","title":"src.features.tfidf.TFIDFVectorizerWrapper.fit_transform","text":"<pre><code>fit_transform(texts: List[Union[str, List[str]]]) -&gt; Tuple\n</code></pre> <p>Fit and transform the texts. Args:     texts: List of cleaned strings, or List of token lists. Returns:     Tuple containing:         - corpus: Sparse TF-IDF matrix         - id2word: Dictionary mapping word IDs to words Raises:     AppException: If input invalid.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def fit_transform(self, texts: List[Union[str, List[str]]]) -&gt; Tuple:\n    \"\"\"\n    Fit and transform the texts.\n    Args:\n        texts: List of cleaned strings, or List of token lists.\n    Returns:\n        Tuple containing:\n            - corpus: Sparse TF-IDF matrix\n            - id2word: Dictionary mapping word IDs to words\n    Raises:\n        AppException: If input invalid.\n    \"\"\"\n    if not texts or not isinstance(texts, list):\n        self.logger.error(\"Input for fit_transform is empty or not a list.\")\n        raise AppException(\"TFIDFVectorizerWrapper.fit_transform: Input text list is empty or not a list.\")\n    matrix = self.vectorizer.fit_transform(texts)\n    self.fitted = True\n\n    # Create id2word dictionary from vocabulary\n    id2word = {id: word for word, id in self.vectorizer.vocabulary_.items()}\n\n    self.logger.info(f\"TF-IDF vectorizer fit and transformed {len(texts)} documents. Shape: {matrix.shape}.\")\n    return matrix, id2word\n</code></pre>"},{"location":"api/#src.features.tfidf.TFIDFVectorizerWrapper.get_feature_names","title":"src.features.tfidf.TFIDFVectorizerWrapper.get_feature_names","text":"<pre><code>get_feature_names() -&gt; List[str]\n</code></pre> <p>Get the feature (vocabulary) names. Returns:     List of feature names. Raises:     AppException: If vectorizer not yet fitted.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def get_feature_names(self) -&gt; List[str]:\n    \"\"\"\n    Get the feature (vocabulary) names.\n    Returns:\n        List of feature names.\n    Raises:\n        AppException: If vectorizer not yet fitted.\n    \"\"\"\n    if not self.fitted:\n        self.logger.error(\"Tried to get feature names from an unfitted vectorizer.\")\n        raise AppException(\"TFIDFVectorizerWrapper: Vectorizer must be fitted before getting feature names.\")\n    return self.vectorizer.get_feature_names_out().tolist()\n</code></pre>"},{"location":"api/#src.features.tfidf.TFIDFVectorizerWrapper.save_vectorizer","title":"src.features.tfidf.TFIDFVectorizerWrapper.save_vectorizer","text":"<pre><code>save_vectorizer(path: str) -&gt; None\n</code></pre> <p>Save the vectorizer to disk using joblib. Args:     path: Path to save the model.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def save_vectorizer(self, path: str) -&gt; None:\n    \"\"\"\n    Save the vectorizer to disk using joblib.\n    Args:\n        path: Path to save the model.\n    \"\"\"\n    joblib.dump(self.vectorizer, path)\n    self.logger.info(f\"TFIDFVectorizer saved to {path}.\")\n</code></pre>"},{"location":"api/#src.features.tfidf.TFIDFVectorizerWrapper.load_vectorizer","title":"src.features.tfidf.TFIDFVectorizerWrapper.load_vectorizer","text":"<pre><code>load_vectorizer(path: str) -&gt; None\n</code></pre> <p>Load a vectorizer from disk using joblib. Args:     path: Path to a saved vectorizer.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def load_vectorizer(self, path: str) -&gt; None:\n    \"\"\"\n    Load a vectorizer from disk using joblib.\n    Args:\n        path: Path to a saved vectorizer.\n    \"\"\"\n    self.vectorizer = joblib.load(path)\n    self.fitted = True\n    self.logger.info(f\"TFIDFVectorizer loaded from {path}.\")\n</code></pre>"},{"location":"api/#src.features.tfidf-functions","title":"Functions","text":""},{"location":"api/#src.models.train","title":"src.models.train","text":""},{"location":"api/#src.models.train-classes","title":"Classes","text":""},{"location":"api/#src.models.train.TopicModelTrainer","title":"src.models.train.TopicModelTrainer","text":"<pre><code>TopicModelTrainer(model_type: str = 'lda', num_topics: int = 10, data_path: str = 'artifacts/preprocessed_bbc_news.csv', vectorizer_type: str = 'tfidf', output_dir: str = 'artifacts/', random_state: int = 42)\n</code></pre> <p>Central training entry point for Topic Modeling (LDA / NMF). Handles data ingestion, feature preparation, model training, and artifact persistence.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def __init__(\n    self,\n    model_type: str = \"lda\",\n    num_topics: int = 10,\n    data_path: str = r\"artifacts/preprocessed_bbc_news.csv\",\n    vectorizer_type: str = \"tfidf\",\n    output_dir: str = \"artifacts/\",\n    random_state: int = 42\n):\n    self.model_type = model_type.lower()\n    self.num_topics = num_topics\n    self.data_path = data_path\n    self.vectorizer_type = vectorizer_type.lower()\n    self.output_dir = output_dir\n    self.random_state = random_state\n\n    self.logger = get_logger(__name__)\n    self.model = None\n    self.vectorizer = None\n    self.corpus = None\n    self.id2word = None\n    self.texts = None\n\n    os.makedirs(self.output_dir, exist_ok=True)\n    self.logger.info(f\"Initialized TopicModelTrainer for {self.model_type.upper()} with {self.num_topics} topics.\")\n</code></pre>"},{"location":"api/#src.models.train.TopicModelTrainer-functions","title":"Functions","text":""},{"location":"api/#src.models.train.TopicModelTrainer.load_data","title":"src.models.train.TopicModelTrainer.load_data","text":"<pre><code>load_data() -&gt; None\n</code></pre> <p>Loads preprocessed text data.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"Loads preprocessed text data.\"\"\"\n    try:\n        if not os.path.exists(self.data_path):\n            raise FileNotFoundError(f\"File not found: {self.data_path}\")\n\n        df = pd.read_csv(self.data_path)\n        if 'cleaned_text' not in df.columns:\n            raise DataValidationError(\"Missing required 'cleaned_text' column in dataset.\")\n\n        # Use cleaned_text column for better topic modeling with valuable words\n        self.texts = df['cleaned_text'].astype(str).tolist()\n        self.logger.info(f\"Loaded {len(self.texts)} documents from {self.data_path}.\")\n    except Exception as e:\n        self.logger.error(f\"Error loading data: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.models.train.TopicModelTrainer.prepare_features","title":"src.models.train.TopicModelTrainer.prepare_features","text":"<pre><code>prepare_features() -&gt; None\n</code></pre> <p>Initializes and fits the chosen vectorizer.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def prepare_features(self) -&gt; None:\n    \"\"\"Initializes and fits the chosen vectorizer.\"\"\"\n    try:\n        if not self.texts:\n            raise DataValidationError(\"No texts available for vectorization.\")\n\n        if self.vectorizer_type == \"tfidf\":\n            self.vectorizer = TFIDFVectorizerWrapper(\n                ngram_range=(1, 2), min_df=2, max_df=0.95, max_features=10000\n            )\n        elif self.vectorizer_type == \"bow\":\n            self.vectorizer = CountVectorizerWrapper(min_df=2, max_df=0.95, ngram_range=(1, 2))\n        else:\n            raise ValueError(f\"Unsupported vectorizer type: {self.vectorizer_type}\")\n\n        matrix, self.id2word = self.vectorizer.fit_transform(self.texts)\n\n        # Convert sparse matrix to gensim corpus format (list of list of tuples)\n        from scipy import sparse\n        import numpy as np\n        from gensim.corpora import Dictionary\n\n        # Convert id2word dict to gensim Dictionary using a safer approach\n        # Create a gensim Dictionary directly from feature names\n        from gensim.corpora import Dictionary\n        feature_names = self.vectorizer.get_feature_names()\n        gensim_dict = Dictionary([feature_names])\n        self.id2word = gensim_dict\n\n        # Convert sparse matrix to gensim corpus format\n        self.corpus = []\n        for i in range(matrix.shape[0]):\n            # Get non-zero elements in this document\n            row = matrix[i].tocoo()\n            # Add as (term_id, term_weight) tuples\n            doc = [(int(term_id), float(weight)) for term_id, weight in zip(row.col, row.data)]\n            self.corpus.append(doc)\n\n        self.logger.info(f\"Vectorization complete using {self.vectorizer_type.upper()} vectorizer.\")\n    except Exception as e:\n        self.logger.error(f\"Feature preparation failed: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.models.train.TopicModelTrainer.train_model","title":"src.models.train.TopicModelTrainer.train_model","text":"<pre><code>train_model() -&gt; None\n</code></pre> <p>Trains either LDA or NMF model based on configuration.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def train_model(self) -&gt; None:\n    \"\"\"Trains either LDA or NMF model based on configuration.\"\"\"\n    try:\n        if self.model_type == \"lda\":\n            self.model = LDAModeler(num_topics=self.num_topics, random_state=self.random_state)\n            self.model.train(self.corpus, self.id2word)\n\n        elif self.model_type == \"nmf\":\n            self.model = NMFModeler(num_topics=self.num_topics, random_state=self.random_state)\n            # NMF needs the TF-IDF matrix and feature names\n            from scipy import sparse\n            import numpy as np\n\n            # Convert corpus back to sparse matrix format for NMF\n            rows = []\n            cols = []\n            data = []\n            for doc_idx, doc in enumerate(self.corpus):\n                for term_id, weight in doc:\n                    rows.append(doc_idx)\n                    cols.append(term_id)\n                    data.append(weight)\n\n            # Create sparse matrix\n            num_docs = len(self.corpus)\n            num_terms = len(self.id2word)\n            tfidf_matrix = sparse.csr_matrix((data, (rows, cols)), shape=(num_docs, num_terms))\n\n            # Get feature names from id2word\n            feature_names = [self.id2word[i] for i in range(len(self.id2word))]\n\n            # Train NMF model\n            self.model.train(tfidf_matrix, feature_names)\n\n        else:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n\n        self.logger.info(f\"{self.model_type.upper()} model training completed successfully.\")\n    except Exception as e:\n        self.logger.error(f\"Model training failed: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.models.train.TopicModelTrainer.save_artifacts","title":"src.models.train.TopicModelTrainer.save_artifacts","text":"<pre><code>save_artifacts() -&gt; None\n</code></pre> <p>Saves trained model, vectorizer, and topic outputs.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def save_artifacts(self) -&gt; None:\n    \"\"\"Saves trained model, vectorizer, and topic outputs.\"\"\"\n    try:\n        model_path = os.path.join(self.output_dir, f\"{self.model_type}_model.pkl\")\n        vectorizer_path = os.path.join(self.output_dir, f\"{self.vectorizer_type}_vectorizer.pkl\")\n\n        # Save model\n        if self.model_type == \"lda\":\n            self.model.save_model(model_path)\n        else:\n            joblib.dump(self.model, model_path)\n\n        # Save vectorizer\n        joblib.dump(self.vectorizer, vectorizer_path)\n        self.logger.info(f\"Saved model and vectorizer artifacts to {self.output_dir}\")\n\n        # Save topics summary\n        topics = self.model.get_topics(num_words=10)\n        topics_path = os.path.join(self.output_dir, f\"{self.model_type}_topics.csv\")\n        pd.DataFrame(topics).to_csv(topics_path, index=False)\n        self.logger.info(f\"Saved topic summary to {topics_path}\")\n    except Exception as e:\n        self.logger.error(f\"Error saving artifacts: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.models.train.TopicModelTrainer.log_training_summary","title":"src.models.train.TopicModelTrainer.log_training_summary","text":"<pre><code>log_training_summary() -&gt; None\n</code></pre> <p>Logs key training metrics and sample topics.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def log_training_summary(self) -&gt; None:\n    \"\"\"Logs key training metrics and sample topics.\"\"\"\n    try:\n        coherence = self.model.compute_coherence_score(self.texts, self.id2word)\n        self.logger.info(f\"Coherence Score (c_v): {coherence:.4f}\")\n\n        if self.model_type == \"lda\":\n            perplexity = self.model.compute_perplexity(self.corpus)\n            self.logger.info(f\"Perplexity: {perplexity:.4f}\")\n\n        top_topics = self.model.get_topics(num_words=8)\n        self.logger.info(\"Top topics and keywords:\")\n        for topic in top_topics:\n            self.logger.info(f\"Topic {topic['topic_id']}: {[w for w, _ in topic['words']]}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to log training summary: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/#src.models.train-functions","title":"Functions","text":""},{"location":"api/#src.utils.logger","title":"src.utils.logger","text":""},{"location":"api/#src.utils.logger-functions","title":"Functions","text":""},{"location":"api/#src.utils.logger.setup_logging","title":"src.utils.logger.setup_logging","text":"<pre><code>setup_logging(log_level: str = 'INFO', log_file: Optional[str] = None, force: bool = False) -&gt; None\n</code></pre> <p>Sets up logging with both console and rotating file handlers. Creates logs/ folder if not existing. If <code>force</code> is True, reconfigures handlers even if already initialized.</p> Source code in <code>src\\utils\\logger.py</code> <pre><code>def setup_logging(log_level: str = \"INFO\", log_file: Optional[str] = None, force: bool = False) -&gt; None:\n    \"\"\"\n    Sets up logging with both console and rotating file handlers.\n    Creates logs/ folder if not existing.\n    If `force` is True, reconfigures handlers even if already initialized.\n    \"\"\"\n    global _initialized\n    if _initialized and not force:\n        return\n\n    logger = logging.getLogger()\n    logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n\n    # Clear existing handlers when forcing reconfiguration\n    if force:\n        for h in list(logger.handlers):\n            logger.removeHandler(h)\n\n    formatter = logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT)\n\n    # Console handler\n    ch = logging.StreamHandler()\n    ch.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    # Rotating file handler\n    # Determine log file path: explicit arg -&gt; env -&gt; default\n    log_file_path = (\n        Path(os.environ.get(\"LOG_FILE_PATH\")) if os.environ.get(\"LOG_FILE_PATH\") else None\n    )\n    if log_file and isinstance(log_file, str):\n        log_file_path = Path(log_file)\n    if log_file_path is None:\n        log_file_path = DEFAULT_LOG_FILE\n\n    log_file_path.parent.mkdir(exist_ok=True, parents=True)\n\n    fh = RotatingFileHandler(str(log_file_path), maxBytes=5*1024*1024, backupCount=5, encoding='utf-8')\n    fh.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n\n    _initialized = True\n</code></pre>"},{"location":"api/#src.utils.logger.get_logger","title":"src.utils.logger.get_logger","text":"<pre><code>get_logger(name: Optional[str] = None) -&gt; logging.Logger\n</code></pre> <p>Returns a logger instance with the given name. Ensures logging is configured. Usage: logger = get_logger(name)</p> Source code in <code>src\\utils\\logger.py</code> <pre><code>def get_logger(name: Optional[str] = None) -&gt; logging.Logger:\n    \"\"\"\n    Returns a logger instance with the given name. Ensures logging is configured.\n    Usage: logger = get_logger(__name__)\n    \"\"\"\n    setup_logging()\n    return logging.getLogger(name)\n</code></pre>"},{"location":"api/data/","title":"Data Processing","text":""},{"location":"api/data/#src.data.data_preprocessing","title":"src.data.data_preprocessing","text":""},{"location":"api/data/#src.data.data_preprocessing-classes","title":"Classes","text":""},{"location":"api/data/#src.data.data_preprocessing.PreprocessingConfig","title":"src.data.data_preprocessing.PreprocessingConfig  <code>dataclass</code>","text":"<pre><code>PreprocessingConfig(text_column: str = 'content', stopwords: List[str] = (lambda: ['said', 'mr', 'say', 'also', 'would', 'one', 'two', 'us'])(), spacy_model: str = 'en_core_web_lg', output_dir: str = 'artifacts/', min_token_length: int = 2, gen_bigrams: bool = True, gen_trigrams: bool = True, logging_level: str = 'INFO', output_file: str = 'preprocessed_bbc_news.csv')\n</code></pre> <p>Configuration for text preprocessing.</p>"},{"location":"api/data/#src.data.data_preprocessing.TextCleaner","title":"src.data.data_preprocessing.TextCleaner","text":"<pre><code>TextCleaner(logger=None)\n</code></pre> <p>Text normalization utilities (lowercase, punctuation, HTML, contractions, etc.)</p> Source code in <code>src\\data\\data_preprocessing.py</code> <pre><code>def __init__(self, logger=None):\n    self.logger = logger or get_logger(__name__)\n    self.CONTRACTIONS = {\"it's\": \"it is\", \"can't\": \"cannot\", \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"}\n</code></pre>"},{"location":"api/data/#src.data.data_preprocessing.TextTokenizer","title":"src.data.data_preprocessing.TextTokenizer","text":"<pre><code>TextTokenizer(nlp, stopwords: set, min_token_length: int = 2, logger=None)\n</code></pre> <p>Tokenization, stopword removal, lemmatization, and n-gram handling.</p> Source code in <code>src\\data\\data_preprocessing.py</code> <pre><code>def __init__(self, nlp, stopwords: set, min_token_length: int = 2, logger=None):\n    self.nlp = nlp\n    self.stopwords = stopwords\n    self.min_token_length = min_token_length\n    self.logger = logger or get_logger(__name__)\n</code></pre>"},{"location":"api/data/#src.data.data_preprocessing.PreprocessingPipeline","title":"src.data.data_preprocessing.PreprocessingPipeline","text":"<pre><code>PreprocessingPipeline(config: PreprocessingConfig, logger=None)\n</code></pre> <p>Orchestrates full text preprocessing: cleaning, tokenization, n-grams. Accepts DataFrame or list.</p> Source code in <code>src\\data\\data_preprocessing.py</code> <pre><code>def __init__(self, config: PreprocessingConfig, logger=None):\n    self.config = config\n    self.logger = logger or get_logger(__name__)\n    try:\n        self.nlp = spacy.load(self.config.spacy_model, disable=[\"parser\", \"ner\"])\n    except OSError:\n        self.logger.error(f\"spaCy model not found: {self.config.spacy_model}. Please run: python -m spacy download {self.config.spacy_model}\")\n        raise AppException(f\"spaCy model not found: {self.config.spacy_model}\")\n    self.stopwords = merge_stopwords(self.config.stopwords, self.nlp)\n    self.cleaner = TextCleaner(self.logger)\n    self.tokenizer = TextTokenizer(self.nlp, self.stopwords, self.config.min_token_length, self.logger)\n</code></pre>"},{"location":"api/data/#src.data.data_preprocessing.TextVisualizer","title":"src.data.data_preprocessing.TextVisualizer","text":"<pre><code>TextVisualizer(logger=None)\n</code></pre> <p>Utility for word frequency, word cloud, and top-n tokens visualization (optional class, extensible for more viz later).</p> Source code in <code>src\\data\\data_preprocessing.py</code> <pre><code>def __init__(self, logger=None):\n    self.logger = logger or get_logger(__name__)\n    try:\n        import matplotlib.pyplot as plt\n        from wordcloud import WordCloud\n        self.plt = plt\n        self.WordCloud = WordCloud\n    except ImportError:\n        self.logger.warning(\"matplotlib or wordcloud not installed; TextVisualizer limited.\")\n        self.plt = None\n        self.WordCloud = None\n</code></pre>"},{"location":"api/data/#src.data.data_preprocessing-functions","title":"Functions","text":""},{"location":"api/features/","title":"Features","text":""},{"location":"api/features/#src.features.tfidf","title":"src.features.tfidf","text":""},{"location":"api/features/#src.features.tfidf-classes","title":"Classes","text":""},{"location":"api/features/#src.features.tfidf.TFIDFVectorizerWrapper","title":"src.features.tfidf.TFIDFVectorizerWrapper","text":"<pre><code>TFIDFVectorizerWrapper(ngram_range: Tuple[int, int] = (1, 2), min_df: int = 2, max_df: Union[int, float] = 0.95, max_features: Optional[int] = 10000, stop_words: Optional[List[str]] = None, logger: Optional[Logger] = None, **kwargs: Any)\n</code></pre> <p>Wrapper for scikit-learn's TfidfVectorizer with logging and configuration. Provides easy integration with text mining/topic modeling pipelines.</p> <p>Initialize the TF-IDF vectorizer wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>ngram_range</code> <code>Tuple[int, int]</code> <p>The lower and upper boundary of the n-grams (default (1,2)).</p> <code>(1, 2)</code> <code>min_df</code> <code>int</code> <p>Minimum document frequency for terms (default 2).</p> <code>2</code> <code>max_df</code> <code>Union[int, float]</code> <p>Max document frequency (int or proportion) (default 0.95).</p> <code>0.95</code> <code>max_features</code> <code>Optional[int]</code> <p>Max number of features (default 10000).</p> <code>10000</code> <code>stop_words</code> <code>Optional[List[str]]</code> <p>List of stopwords (optional, overrides TfidfVectorizer's built-ins).</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger instance.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Other parameters for scikit-learn TfidfVectorizer.</p> <code>{}</code> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def __init__(\n    self,\n    ngram_range: Tuple[int, int] = (1, 2),\n    min_df: int = 2,\n    max_df: Union[int, float] = 0.95,\n    max_features: Optional[int] = 10000,\n    stop_words: Optional[List[str]] = None,\n    logger: Optional[logging.Logger] = None,\n    **kwargs: Any\n):\n    \"\"\"\n    Initialize the TF-IDF vectorizer wrapper.\n\n    Args:\n        ngram_range: The lower and upper boundary of the n-grams (default (1,2)).\n        min_df: Minimum document frequency for terms (default 2).\n        max_df: Max document frequency (int or proportion) (default 0.95).\n        max_features: Max number of features (default 10000).\n        stop_words: List of stopwords (optional, overrides TfidfVectorizer's built-ins).\n        logger: Optional logger instance.\n        **kwargs: Other parameters for scikit-learn TfidfVectorizer.\n    \"\"\"\n    self.logger = logger or get_logger(__name__)\n    self.vectorizer = TfidfVectorizer(\n        ngram_range=ngram_range,\n        min_df=min_df,\n        max_df=max_df,\n        max_features=max_features,\n        stop_words=stop_words,\n        tokenizer=custom_tokenizer,\n        preprocessor=custom_preprocessor,\n        token_pattern=None,\n        **kwargs\n    )\n    self.fitted = False\n    self.logger.info(f\"Initialized TFIDFVectorizerWrapper with params: ngram_range={ngram_range}, min_df={min_df}, max_df={max_df}, max_features={max_features}\")\n</code></pre>"},{"location":"api/features/#src.features.tfidf.TFIDFVectorizerWrapper-functions","title":"Functions","text":""},{"location":"api/features/#src.features.tfidf.TFIDFVectorizerWrapper.fit","title":"src.features.tfidf.TFIDFVectorizerWrapper.fit","text":"<pre><code>fit(texts: List[Union[str, List[str]]]) -&gt; TFIDFVectorizerWrapper\n</code></pre> <p>Fit the vectorizer to a corpus of texts. Args:     texts: List of cleaned strings, or List of token lists. Returns:     self Raises:     AppException: If input is empty or invalid.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def fit(self, texts: List[Union[str, List[str]]]) -&gt; 'TFIDFVectorizerWrapper':\n    \"\"\"\n    Fit the vectorizer to a corpus of texts.\n    Args:\n        texts: List of cleaned strings, or List of token lists.\n    Returns:\n        self\n    Raises:\n        AppException: If input is empty or invalid.\n    \"\"\"\n    if not texts or not isinstance(texts, list):\n        self.logger.error(\"Input for fit is empty or not a list.\")\n        raise AppException(\"TFIDFVectorizerWrapper.fit: Input text list is empty or not a list.\")\n    self.vectorizer.fit(texts)\n    self.fitted = True\n    self.logger.info(f\"TF-IDF vectorizer fitted on {len(texts)} documents. Vocabulary size: {len(self.vectorizer.vocabulary_)}.\")\n    return self\n</code></pre>"},{"location":"api/features/#src.features.tfidf.TFIDFVectorizerWrapper.transform","title":"src.features.tfidf.TFIDFVectorizerWrapper.transform","text":"<pre><code>transform(texts: List[Union[str, List[str]]])\n</code></pre> <p>Transform new texts to the vectorized space. Args:     texts: List of cleaned strings, or List of token lists. Returns:     Sparse TF-IDF matrix Raises:     AppException: If vectorizer not fitted or input invalid.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def transform(self, texts: List[Union[str, List[str]]]):\n    \"\"\"\n    Transform new texts to the vectorized space.\n    Args:\n        texts: List of cleaned strings, or List of token lists.\n    Returns:\n        Sparse TF-IDF matrix\n    Raises:\n        AppException: If vectorizer not fitted or input invalid.\n    \"\"\"\n    if not texts or not isinstance(texts, list):\n        self.logger.error(\"Input for transform is empty or not a list.\")\n        raise AppException(\"TFIDFVectorizerWrapper.transform: Input text list is empty or not a list.\")\n    try:\n        matrix = self.vectorizer.transform(texts)\n        self.logger.info(f\"Transformed {len(texts)} documents. Resulting shape: {matrix.shape}.\")\n        return matrix\n    except NotFittedError as e:\n        self.logger.error(\"TF-IDF vectorizer not fitted.\")\n        raise AppException(\"TFIDFVectorizerWrapper: Vectorizer must be fitted before use.\") from e\n</code></pre>"},{"location":"api/features/#src.features.tfidf.TFIDFVectorizerWrapper.fit_transform","title":"src.features.tfidf.TFIDFVectorizerWrapper.fit_transform","text":"<pre><code>fit_transform(texts: List[Union[str, List[str]]]) -&gt; Tuple\n</code></pre> <p>Fit and transform the texts. Args:     texts: List of cleaned strings, or List of token lists. Returns:     Tuple containing:         - corpus: Sparse TF-IDF matrix         - id2word: Dictionary mapping word IDs to words Raises:     AppException: If input invalid.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def fit_transform(self, texts: List[Union[str, List[str]]]) -&gt; Tuple:\n    \"\"\"\n    Fit and transform the texts.\n    Args:\n        texts: List of cleaned strings, or List of token lists.\n    Returns:\n        Tuple containing:\n            - corpus: Sparse TF-IDF matrix\n            - id2word: Dictionary mapping word IDs to words\n    Raises:\n        AppException: If input invalid.\n    \"\"\"\n    if not texts or not isinstance(texts, list):\n        self.logger.error(\"Input for fit_transform is empty or not a list.\")\n        raise AppException(\"TFIDFVectorizerWrapper.fit_transform: Input text list is empty or not a list.\")\n    matrix = self.vectorizer.fit_transform(texts)\n    self.fitted = True\n\n    # Create id2word dictionary from vocabulary\n    id2word = {id: word for word, id in self.vectorizer.vocabulary_.items()}\n\n    self.logger.info(f\"TF-IDF vectorizer fit and transformed {len(texts)} documents. Shape: {matrix.shape}.\")\n    return matrix, id2word\n</code></pre>"},{"location":"api/features/#src.features.tfidf.TFIDFVectorizerWrapper.get_feature_names","title":"src.features.tfidf.TFIDFVectorizerWrapper.get_feature_names","text":"<pre><code>get_feature_names() -&gt; List[str]\n</code></pre> <p>Get the feature (vocabulary) names. Returns:     List of feature names. Raises:     AppException: If vectorizer not yet fitted.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def get_feature_names(self) -&gt; List[str]:\n    \"\"\"\n    Get the feature (vocabulary) names.\n    Returns:\n        List of feature names.\n    Raises:\n        AppException: If vectorizer not yet fitted.\n    \"\"\"\n    if not self.fitted:\n        self.logger.error(\"Tried to get feature names from an unfitted vectorizer.\")\n        raise AppException(\"TFIDFVectorizerWrapper: Vectorizer must be fitted before getting feature names.\")\n    return self.vectorizer.get_feature_names_out().tolist()\n</code></pre>"},{"location":"api/features/#src.features.tfidf.TFIDFVectorizerWrapper.save_vectorizer","title":"src.features.tfidf.TFIDFVectorizerWrapper.save_vectorizer","text":"<pre><code>save_vectorizer(path: str) -&gt; None\n</code></pre> <p>Save the vectorizer to disk using joblib. Args:     path: Path to save the model.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def save_vectorizer(self, path: str) -&gt; None:\n    \"\"\"\n    Save the vectorizer to disk using joblib.\n    Args:\n        path: Path to save the model.\n    \"\"\"\n    joblib.dump(self.vectorizer, path)\n    self.logger.info(f\"TFIDFVectorizer saved to {path}.\")\n</code></pre>"},{"location":"api/features/#src.features.tfidf.TFIDFVectorizerWrapper.load_vectorizer","title":"src.features.tfidf.TFIDFVectorizerWrapper.load_vectorizer","text":"<pre><code>load_vectorizer(path: str) -&gt; None\n</code></pre> <p>Load a vectorizer from disk using joblib. Args:     path: Path to a saved vectorizer.</p> Source code in <code>src\\features\\tfidf.py</code> <pre><code>def load_vectorizer(self, path: str) -&gt; None:\n    \"\"\"\n    Load a vectorizer from disk using joblib.\n    Args:\n        path: Path to a saved vectorizer.\n    \"\"\"\n    self.vectorizer = joblib.load(path)\n    self.fitted = True\n    self.logger.info(f\"TFIDFVectorizer loaded from {path}.\")\n</code></pre>"},{"location":"api/features/#src.features.tfidf-functions","title":"Functions","text":""},{"location":"api/models/","title":"Models","text":""},{"location":"api/models/#src.models.train","title":"src.models.train","text":""},{"location":"api/models/#src.models.train-classes","title":"Classes","text":""},{"location":"api/models/#src.models.train.TopicModelTrainer","title":"src.models.train.TopicModelTrainer","text":"<pre><code>TopicModelTrainer(model_type: str = 'lda', num_topics: int = 10, data_path: str = 'artifacts/preprocessed_bbc_news.csv', vectorizer_type: str = 'tfidf', output_dir: str = 'artifacts/', random_state: int = 42)\n</code></pre> <p>Central training entry point for Topic Modeling (LDA / NMF). Handles data ingestion, feature preparation, model training, and artifact persistence.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def __init__(\n    self,\n    model_type: str = \"lda\",\n    num_topics: int = 10,\n    data_path: str = r\"artifacts/preprocessed_bbc_news.csv\",\n    vectorizer_type: str = \"tfidf\",\n    output_dir: str = \"artifacts/\",\n    random_state: int = 42\n):\n    self.model_type = model_type.lower()\n    self.num_topics = num_topics\n    self.data_path = data_path\n    self.vectorizer_type = vectorizer_type.lower()\n    self.output_dir = output_dir\n    self.random_state = random_state\n\n    self.logger = get_logger(__name__)\n    self.model = None\n    self.vectorizer = None\n    self.corpus = None\n    self.id2word = None\n    self.texts = None\n\n    os.makedirs(self.output_dir, exist_ok=True)\n    self.logger.info(f\"Initialized TopicModelTrainer for {self.model_type.upper()} with {self.num_topics} topics.\")\n</code></pre>"},{"location":"api/models/#src.models.train.TopicModelTrainer-functions","title":"Functions","text":""},{"location":"api/models/#src.models.train.TopicModelTrainer.load_data","title":"src.models.train.TopicModelTrainer.load_data","text":"<pre><code>load_data() -&gt; None\n</code></pre> <p>Loads preprocessed text data.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"Loads preprocessed text data.\"\"\"\n    try:\n        if not os.path.exists(self.data_path):\n            raise FileNotFoundError(f\"File not found: {self.data_path}\")\n\n        df = pd.read_csv(self.data_path)\n        if 'cleaned_text' not in df.columns:\n            raise DataValidationError(\"Missing required 'cleaned_text' column in dataset.\")\n\n        # Use cleaned_text column for better topic modeling with valuable words\n        self.texts = df['cleaned_text'].astype(str).tolist()\n        self.logger.info(f\"Loaded {len(self.texts)} documents from {self.data_path}.\")\n    except Exception as e:\n        self.logger.error(f\"Error loading data: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/models/#src.models.train.TopicModelTrainer.prepare_features","title":"src.models.train.TopicModelTrainer.prepare_features","text":"<pre><code>prepare_features() -&gt; None\n</code></pre> <p>Initializes and fits the chosen vectorizer.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def prepare_features(self) -&gt; None:\n    \"\"\"Initializes and fits the chosen vectorizer.\"\"\"\n    try:\n        if not self.texts:\n            raise DataValidationError(\"No texts available for vectorization.\")\n\n        if self.vectorizer_type == \"tfidf\":\n            self.vectorizer = TFIDFVectorizerWrapper(\n                ngram_range=(1, 2), min_df=2, max_df=0.95, max_features=10000\n            )\n        elif self.vectorizer_type == \"bow\":\n            self.vectorizer = CountVectorizerWrapper(min_df=2, max_df=0.95, ngram_range=(1, 2))\n        else:\n            raise ValueError(f\"Unsupported vectorizer type: {self.vectorizer_type}\")\n\n        matrix, self.id2word = self.vectorizer.fit_transform(self.texts)\n\n        # Convert sparse matrix to gensim corpus format (list of list of tuples)\n        from scipy import sparse\n        import numpy as np\n        from gensim.corpora import Dictionary\n\n        # Convert id2word dict to gensim Dictionary using a safer approach\n        # Create a gensim Dictionary directly from feature names\n        from gensim.corpora import Dictionary\n        feature_names = self.vectorizer.get_feature_names()\n        gensim_dict = Dictionary([feature_names])\n        self.id2word = gensim_dict\n\n        # Convert sparse matrix to gensim corpus format\n        self.corpus = []\n        for i in range(matrix.shape[0]):\n            # Get non-zero elements in this document\n            row = matrix[i].tocoo()\n            # Add as (term_id, term_weight) tuples\n            doc = [(int(term_id), float(weight)) for term_id, weight in zip(row.col, row.data)]\n            self.corpus.append(doc)\n\n        self.logger.info(f\"Vectorization complete using {self.vectorizer_type.upper()} vectorizer.\")\n    except Exception as e:\n        self.logger.error(f\"Feature preparation failed: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/models/#src.models.train.TopicModelTrainer.train_model","title":"src.models.train.TopicModelTrainer.train_model","text":"<pre><code>train_model() -&gt; None\n</code></pre> <p>Trains either LDA or NMF model based on configuration.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def train_model(self) -&gt; None:\n    \"\"\"Trains either LDA or NMF model based on configuration.\"\"\"\n    try:\n        if self.model_type == \"lda\":\n            self.model = LDAModeler(num_topics=self.num_topics, random_state=self.random_state)\n            self.model.train(self.corpus, self.id2word)\n\n        elif self.model_type == \"nmf\":\n            self.model = NMFModeler(num_topics=self.num_topics, random_state=self.random_state)\n            # NMF needs the TF-IDF matrix and feature names\n            from scipy import sparse\n            import numpy as np\n\n            # Convert corpus back to sparse matrix format for NMF\n            rows = []\n            cols = []\n            data = []\n            for doc_idx, doc in enumerate(self.corpus):\n                for term_id, weight in doc:\n                    rows.append(doc_idx)\n                    cols.append(term_id)\n                    data.append(weight)\n\n            # Create sparse matrix\n            num_docs = len(self.corpus)\n            num_terms = len(self.id2word)\n            tfidf_matrix = sparse.csr_matrix((data, (rows, cols)), shape=(num_docs, num_terms))\n\n            # Get feature names from id2word\n            feature_names = [self.id2word[i] for i in range(len(self.id2word))]\n\n            # Train NMF model\n            self.model.train(tfidf_matrix, feature_names)\n\n        else:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n\n        self.logger.info(f\"{self.model_type.upper()} model training completed successfully.\")\n    except Exception as e:\n        self.logger.error(f\"Model training failed: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/models/#src.models.train.TopicModelTrainer.save_artifacts","title":"src.models.train.TopicModelTrainer.save_artifacts","text":"<pre><code>save_artifacts() -&gt; None\n</code></pre> <p>Saves trained model, vectorizer, and topic outputs.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def save_artifacts(self) -&gt; None:\n    \"\"\"Saves trained model, vectorizer, and topic outputs.\"\"\"\n    try:\n        model_path = os.path.join(self.output_dir, f\"{self.model_type}_model.pkl\")\n        vectorizer_path = os.path.join(self.output_dir, f\"{self.vectorizer_type}_vectorizer.pkl\")\n\n        # Save model\n        if self.model_type == \"lda\":\n            self.model.save_model(model_path)\n        else:\n            joblib.dump(self.model, model_path)\n\n        # Save vectorizer\n        joblib.dump(self.vectorizer, vectorizer_path)\n        self.logger.info(f\"Saved model and vectorizer artifacts to {self.output_dir}\")\n\n        # Save topics summary\n        topics = self.model.get_topics(num_words=10)\n        topics_path = os.path.join(self.output_dir, f\"{self.model_type}_topics.csv\")\n        pd.DataFrame(topics).to_csv(topics_path, index=False)\n        self.logger.info(f\"Saved topic summary to {topics_path}\")\n    except Exception as e:\n        self.logger.error(f\"Error saving artifacts: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/models/#src.models.train.TopicModelTrainer.log_training_summary","title":"src.models.train.TopicModelTrainer.log_training_summary","text":"<pre><code>log_training_summary() -&gt; None\n</code></pre> <p>Logs key training metrics and sample topics.</p> Source code in <code>src\\models\\train.py</code> <pre><code>def log_training_summary(self) -&gt; None:\n    \"\"\"Logs key training metrics and sample topics.\"\"\"\n    try:\n        coherence = self.model.compute_coherence_score(self.texts, self.id2word)\n        self.logger.info(f\"Coherence Score (c_v): {coherence:.4f}\")\n\n        if self.model_type == \"lda\":\n            perplexity = self.model.compute_perplexity(self.corpus)\n            self.logger.info(f\"Perplexity: {perplexity:.4f}\")\n\n        top_topics = self.model.get_topics(num_words=8)\n        self.logger.info(\"Top topics and keywords:\")\n        for topic in top_topics:\n            self.logger.info(f\"Topic {topic['topic_id']}: {[w for w, _ in topic['words']]}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to log training summary: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/models/#src.models.train-functions","title":"Functions","text":""},{"location":"api/topics/","title":"Topic Models","text":""},{"location":"api/topics/#src.topics.lda_model","title":"src.topics.lda_model","text":""},{"location":"api/topics/#src.topics.lda_model-classes","title":"Classes","text":""},{"location":"api/topics/#src.topics.lda_model.LDAModeler","title":"src.topics.lda_model.LDAModeler","text":"<pre><code>LDAModeler(num_topics: int = 10, passes: int = 10, chunksize: int = 100, alpha: str = 'auto', random_state: int = 42)\n</code></pre> <p>LDA Topic Modeling wrapper using gensim.models.LdaModel. Provides training, evaluation, topic extraction, and persistence utilities with robust logging.</p> <p>Initializes the LDA Modeler. Args:     num_topics: Number of topics for LDA.     passes: Number of passes over corpus during training.     chunksize: Number of documents to use in each training chunk.     alpha: Document-topic density prior.     random_state: Random state for reproducibility.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def __init__(\n    self,\n    num_topics: int = 10,\n    passes: int = 10,\n    chunksize: int = 100,\n    alpha: str = 'auto',\n    random_state: int = 42\n) -&gt; None:\n    \"\"\"\n    Initializes the LDA Modeler.\n    Args:\n        num_topics: Number of topics for LDA.\n        passes: Number of passes over corpus during training.\n        chunksize: Number of documents to use in each training chunk.\n        alpha: Document-topic density prior.\n        random_state: Random state for reproducibility.\n    \"\"\"\n    self.num_topics = num_topics\n    self.passes = passes\n    self.chunksize = chunksize\n    self.alpha = alpha\n    self.random_state = random_state\n    self.model: Optional[LdaModel] = None\n    self.logger = get_logger(__name__)\n    self.logger.info(f\"LDAModeler initialized with num_topics={num_topics}, passes={passes}, chunksize={chunksize}, alpha={alpha}.\")\n</code></pre>"},{"location":"api/topics/#src.topics.lda_model.LDAModeler-functions","title":"Functions","text":""},{"location":"api/topics/#src.topics.lda_model.LDAModeler.train","title":"src.topics.lda_model.LDAModeler.train","text":"<pre><code>train(corpus: List[List[tuple]], id2word: Dictionary) -&gt; None\n</code></pre> <p>Fits the LDA model to the provided corpus. Args:     corpus: Gensim corpus (list of bag-of-words).     id2word: Gensim dictionary object. Raises:     DataValidationError: On invalid corpus/dictionary.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def train(self, corpus: List[List[tuple]], id2word: Dictionary) -&gt; None:\n    \"\"\"\n    Fits the LDA model to the provided corpus.\n    Args:\n        corpus: Gensim corpus (list of bag-of-words).\n        id2word: Gensim dictionary object.\n    Raises:\n        DataValidationError: On invalid corpus/dictionary.\n    \"\"\"\n    try:\n        if not corpus or not isinstance(corpus, list):\n            raise DataValidationError(\"Invalid or empty corpus provided.\")\n        if not isinstance(id2word, Dictionary):\n            raise DataValidationError(\"id2word must be a gensim Dictionary object.\")\n        self.logger.info(\"Starting LDA model training...\")\n        self.model = LdaModel(\n            corpus=corpus,\n            id2word=id2word,\n            num_topics=self.num_topics,\n            passes=self.passes,\n            chunksize=self.chunksize,\n            alpha=self.alpha,\n            random_state=self.random_state\n        )\n        self.logger.info(f\"LDA model training complete. Number of topics: {self.model.num_topics}\")\n    except Exception as e:\n        self.logger.error(f\"Error during LDA model training: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/topics/#src.topics.lda_model.LDAModeler.get_topics","title":"src.topics.lda_model.LDAModeler.get_topics","text":"<pre><code>get_topics(num_words: int = 10) -&gt; List[Dict[str, Union[int, List[Tuple[str, float]]]]]\n</code></pre> <p>Returns the top words for each topic. Args:     num_words: Number of top words per topic. Returns:     List of topics, each as dict of topic_id and (word, weight) tuples. Raises:     AppException: If model is not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def get_topics(self, num_words: int = 10) -&gt; List[Dict[str, Union[int, List[Tuple[str, float]]]]]:\n    \"\"\"\n    Returns the top words for each topic.\n    Args:\n        num_words: Number of top words per topic.\n    Returns:\n        List of topics, each as dict of topic_id and (word, weight) tuples.\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained yet.\")\n        topics = []\n        for idx, topic in self.model.show_topics(num_topics=self.num_topics, num_words=num_words, formatted=False):\n            topics.append({\n                \"topic_id\": idx,\n                \"words\": [(word, round(weight, 4)) for word, weight in topic]\n            })\n        self.logger.info(f\"Extracted top {num_words} words for each topic.\")\n        return topics\n    except Exception as e:\n        self.logger.error(f\"Failed to get topic words: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/topics/#src.topics.lda_model.LDAModeler.compute_coherence_score","title":"src.topics.lda_model.LDAModeler.compute_coherence_score","text":"<pre><code>compute_coherence_score(texts: List[List[str]], id2word: Dictionary, coherence: str = 'c_v') -&gt; float\n</code></pre> <p>Compute the coherence score for trained topics. Args:     texts: Tokenized texts for coherence computation.     id2word: Gensim dictionary object.     coherence: Coherence type (default: 'c_v'). Returns:     Coherence score (float). Raises:     AppException: On error or if model not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def compute_coherence_score(self, texts: List[List[str]], id2word: Dictionary, coherence: str = 'c_v') -&gt; float:\n    \"\"\"\n    Compute the coherence score for trained topics.\n    Args:\n        texts: Tokenized texts for coherence computation.\n        id2word: Gensim dictionary object.\n        coherence: Coherence type (default: 'c_v').\n    Returns:\n        Coherence score (float).\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained.\")\n        coherence_model = CoherenceModel(\n            model=self.model,\n            texts=texts,\n            dictionary=id2word,\n            coherence=coherence\n        )\n        score = coherence_model.get_coherence()\n        self.logger.info(f\"Coherence ({coherence}) score: {score:.4f}\")\n        return score\n    except Exception as e:\n        self.logger.error(f\"Failed to compute coherence score: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/topics/#src.topics.lda_model.LDAModeler.compute_perplexity","title":"src.topics.lda_model.LDAModeler.compute_perplexity","text":"<pre><code>compute_perplexity(corpus: List[List[tuple]]) -&gt; float\n</code></pre> <p>Compute model perplexity on the given corpus. Args:     corpus: Corpus to compute perplexity against. Returns:     Perplexity value (float). Raises:     AppException: On error or if model not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def compute_perplexity(self, corpus: List[List[tuple]]) -&gt; float:\n    \"\"\"\n    Compute model perplexity on the given corpus.\n    Args:\n        corpus: Corpus to compute perplexity against.\n    Returns:\n        Perplexity value (float).\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained.\")\n        perplexity = self.model.log_perplexity(corpus)\n        self.logger.info(f\"Model log perplexity: {perplexity:.4f}\")\n        return perplexity\n    except Exception as e:\n        self.logger.error(f\"Failed to compute perplexity: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/topics/#src.topics.lda_model.LDAModeler.get_dominant_topic_per_doc","title":"src.topics.lda_model.LDAModeler.get_dominant_topic_per_doc","text":"<pre><code>get_dominant_topic_per_doc(corpus: List[List[tuple]]) -&gt; List[int]\n</code></pre> <p>Returns the most likely topic for each document in the corpus. Args:     corpus: Corpus in BOW format. Returns:     List of dominant topic indices per document. Raises:     AppException: On error or if model not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def get_dominant_topic_per_doc(self, corpus: List[List[tuple]]) -&gt; List[int]:\n    \"\"\"\n    Returns the most likely topic for each document in the corpus.\n    Args:\n        corpus: Corpus in BOW format.\n    Returns:\n        List of dominant topic indices per document.\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained.\")\n        dominant_topics = []\n        for doc_bow in corpus:\n            topic_probs = self.model.get_document_topics(doc_bow)\n            if topic_probs:\n                dominant_topic = max(topic_probs, key=lambda tup: tup[1])[0]\n            else:\n                dominant_topic = -1\n            dominant_topics.append(dominant_topic)\n        self.logger.info(\"Extracted dominant topic for each document.\")\n        return dominant_topics\n    except Exception as e:\n        self.logger.error(f\"Failed to extract dominant topics: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/topics/#src.topics.lda_model.LDAModeler.save_model","title":"src.topics.lda_model.LDAModeler.save_model","text":"<pre><code>save_model(path: str) -&gt; None\n</code></pre> <p>Saves the trained model to a file. Args:     path: Path to save model. Raises:     AppException: On error or if model not trained.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def save_model(self, path: str) -&gt; None:\n    \"\"\"\n    Saves the trained model to a file.\n    Args:\n        path: Path to save model.\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"LDA model is not trained and cannot be saved.\")\n        self.model.save(path)\n        self.logger.info(f\"LDA model saved to: {path}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to save LDA model: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/topics/#src.topics.lda_model.LDAModeler.load_model","title":"src.topics.lda_model.LDAModeler.load_model","text":"<pre><code>load_model(path: str) -&gt; None\n</code></pre> <p>Loads a model from the given path. Args:     path: Path to load model from. Raises:     AppException: On error loading the model.</p> Source code in <code>src\\topics\\lda_model.py</code> <pre><code>def load_model(self, path: str) -&gt; None:\n    \"\"\"\n    Loads a model from the given path.\n    Args:\n        path: Path to load model from.\n    Raises:\n        AppException: On error loading the model.\n    \"\"\"\n    try:\n        self.model = LdaModel.load(path)\n        self.logger.info(f\"LDA model loaded from: {path}\")\n    except Exception as e:\n        self.logger.error(f\"Failed to load LDA model: {e}\")\n        raise AppException(str(e))\n</code></pre>"},{"location":"api/topics/#src.topics.lda_model-functions","title":"Functions","text":""},{"location":"api/topics/#src.topics.nmf_model","title":"src.topics.nmf_model","text":""},{"location":"api/topics/#src.topics.nmf_model-classes","title":"Classes","text":""},{"location":"api/topics/#src.topics.nmf_model.NMFModeler","title":"src.topics.nmf_model.NMFModeler","text":"<pre><code>NMFModeler(num_topics: int = 10, init: str = 'nndsvda', max_iter: int = 500, random_state: int = 42, alpha: float = 0.1, l1_ratio: float = 0.5)\n</code></pre> <p>Non-Negative Matrix Factorization (NMF) Topic Modeling wrapper using  sklearn.decomposition.NMF. Provides training, evaluation, topic extraction,  visualization, and persistence utilities with robust logging and error handling.</p> <p>Initializes the NMF Modeler.</p> <p>Parameters:</p> Name Type Description Default <code>num_topics</code> <code>int</code> <p>Number of topics to extract.</p> <code>10</code> <code>init</code> <code>str</code> <p>Initialization method ('random', 'nndsvd', 'nndsvda', 'nndsvdar').</p> <code>'nndsvda'</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>500</code> <code>random_state</code> <code>int</code> <p>Random state for reproducibility.</p> <code>42</code> <code>alpha</code> <code>float</code> <p>Regularization parameter (multiplier for regularization terms).</p> <code>0.1</code> <code>l1_ratio</code> <code>float</code> <p>Ratio of L1 to L2 regularization (0.0 = L2 only, 1.0 = L1 only).</p> <code>0.5</code> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def __init__(\n    self,\n    num_topics: int = 10,\n    init: str = 'nndsvda',\n    max_iter: int = 500,\n    random_state: int = 42,\n    alpha: float = 0.1,\n    l1_ratio: float = 0.5\n) -&gt; None:\n    \"\"\"\n    Initializes the NMF Modeler.\n\n    Args:\n        num_topics: Number of topics to extract.\n        init: Initialization method ('random', 'nndsvd', 'nndsvda', 'nndsvdar').\n        max_iter: Maximum number of iterations.\n        random_state: Random state for reproducibility.\n        alpha: Regularization parameter (multiplier for regularization terms).\n        l1_ratio: Ratio of L1 to L2 regularization (0.0 = L2 only, 1.0 = L1 only).\n    \"\"\"\n    self.num_topics = num_topics\n    self.init = init\n    self.max_iter = max_iter\n    self.random_state = random_state\n    self.alpha = alpha\n    self.l1_ratio = l1_ratio\n\n    self.model: Optional[NMF] = None\n    self.feature_names: Optional[List[str]] = None\n    self.document_topic_matrix: Optional[np.ndarray] = None\n    self.topic_term_matrix: Optional[np.ndarray] = None\n\n    self.logger = get_logger(__name__)\n    self.logger.info(\n        f\"NMFModeler initialized with num_topics={num_topics}, init={init}, \"\n        f\"max_iter={max_iter}, alpha={alpha}, l1_ratio={l1_ratio}\"\n    )\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model.NMFModeler-functions","title":"Functions","text":""},{"location":"api/topics/#src.topics.nmf_model.NMFModeler.train","title":"src.topics.nmf_model.NMFModeler.train","text":"<pre><code>train(tfidf_matrix: Union[ndarray, csr_matrix], feature_names: List[str]) -&gt; None\n</code></pre> <p>Fits the NMF model to the provided TF-IDF matrix.</p> <p>Parameters:</p> Name Type Description Default <code>tfidf_matrix</code> <code>Union[ndarray, csr_matrix]</code> <p>TF-IDF matrix (can be sparse or dense).</p> required <code>feature_names</code> <code>List[str]</code> <p>List of feature names (vocabulary).</p> required <p>Raises:</p> Type Description <code>DataValidationError</code> <p>On invalid input data.</p> <code>AppException</code> <p>On training errors.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def train(\n    self, \n    tfidf_matrix: Union[np.ndarray, csr_matrix], \n    feature_names: List[str]\n) -&gt; None:\n    \"\"\"\n    Fits the NMF model to the provided TF-IDF matrix.\n\n    Args:\n        tfidf_matrix: TF-IDF matrix (can be sparse or dense).\n        feature_names: List of feature names (vocabulary).\n\n    Raises:\n        DataValidationError: On invalid input data.\n        AppException: On training errors.\n    \"\"\"\n    try:\n        # Validate inputs\n        if tfidf_matrix is None or (isinstance(tfidf_matrix, np.ndarray) and tfidf_matrix.size == 0):\n            raise DataValidationError(\"TF-IDF matrix is empty or None.\")\n\n        if not feature_names or not isinstance(feature_names, list):\n            raise DataValidationError(\"feature_names must be a non-empty list.\")\n\n        # Convert to sparse matrix if dense\n        if isinstance(tfidf_matrix, np.ndarray):\n            self.logger.info(\"Converting dense matrix to sparse format for efficiency.\")\n            tfidf_matrix = csr_matrix(tfidf_matrix)\n\n        # Validate matrix dimensions\n        if tfidf_matrix.shape[1] != len(feature_names):\n            raise DataValidationError(\n                f\"Matrix columns ({tfidf_matrix.shape[1]}) must match \"\n                f\"feature_names length ({len(feature_names)}).\"\n            )\n\n        self.feature_names = feature_names\n\n        self.logger.info(\n            f\"Starting NMF training on matrix of shape {tfidf_matrix.shape}...\"\n        )\n\n        # Initialize and train NMF model\n        self.model = NMF(\n            n_components=self.num_topics,\n            init=self.init,\n            max_iter=self.max_iter,\n            random_state=self.random_state,\n            alpha_W=self.alpha,  # Changed from alpha to alpha_W\n            l1_ratio=self.l1_ratio,\n            verbose=0\n        )\n\n        # Fit and transform\n        self.document_topic_matrix = self.model.fit_transform(tfidf_matrix)\n        self.topic_term_matrix = self.model.components_\n\n        self.logger.info(\n            f\"NMF training complete. Reconstruction error: \"\n            f\"{self.model.reconstruction_err_:.4f}\"\n        )\n        self.logger.info(\n            f\"Document-topic matrix shape: {self.document_topic_matrix.shape}\"\n        )\n        self.logger.info(\n            f\"Topic-term matrix shape: {self.topic_term_matrix.shape}\"\n        )\n\n    except DataValidationError as e:\n        self.logger.error(f\"Data validation error during NMF training: {e}\")\n        raise\n    except Exception as e:\n        self.logger.error(f\"Error during NMF model training: {e}\")\n        raise AppException(f\"NMF training failed: {str(e)}\")\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model.NMFModeler.get_topics","title":"src.topics.nmf_model.NMFModeler.get_topics","text":"<pre><code>get_topics(num_words: int = 10) -&gt; List[Dict[str, Union[int, List[Tuple[str, float]]]]]\n</code></pre> <p>Returns the top words for each topic.</p> <p>Parameters:</p> Name Type Description Default <code>num_words</code> <code>int</code> <p>Number of top words per topic.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Union[int, List[Tuple[str, float]]]]]</code> <p>List of topics, each as dict with topic_id and (word, weight) tuples.</p> <p>Raises:</p> Type Description <code>AppException</code> <p>If model is not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def get_topics(\n    self, \n    num_words: int = 10\n) -&gt; List[Dict[str, Union[int, List[Tuple[str, float]]]]]:\n    \"\"\"\n    Returns the top words for each topic.\n\n    Args:\n        num_words: Number of top words per topic.\n\n    Returns:\n        List of topics, each as dict with topic_id and (word, weight) tuples.\n\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.model is None or self.topic_term_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        if self.feature_names is None:\n            raise AppException(\"Feature names not available.\")\n\n        topics = []\n\n        for topic_idx in range(self.num_topics):\n            # Get top word indices for this topic\n            top_indices = self.topic_term_matrix[topic_idx].argsort()[-num_words:][::-1]\n            top_words = [\n                (self.feature_names[i], float(self.topic_term_matrix[topic_idx][i]))\n                for i in top_indices\n            ]\n\n            topics.append({\n                \"topic_id\": topic_idx,\n                \"words\": [(word, round(weight, 4)) for word, weight in top_words]\n            })\n\n            # Log top words\n            words_str = \", \".join([f\"{word}({weight:.3f})\" for word, weight in top_words[:5]])\n            self.logger.info(f\"Topic {topic_idx}: {words_str}...\")\n\n        self.logger.info(f\"Extracted top {num_words} words for {self.num_topics} topics.\")\n        return topics\n\n    except AppException as e:\n        self.logger.error(f\"Failed to get topic words: {e}\")\n        raise\n    except Exception as e:\n        self.logger.error(f\"Unexpected error in get_topics: {e}\")\n        raise AppException(f\"Failed to extract topics: {str(e)}\")\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model.NMFModeler.get_document_topics","title":"src.topics.nmf_model.NMFModeler.get_document_topics","text":"<pre><code>get_document_topics() -&gt; List[Dict[str, Union[int, float]]]\n</code></pre> <p>Returns the dominant topic for each document along with topic distribution.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Union[int, float]]]</code> <p>List of dicts containing dominant_topic, probability, and full distribution.</p> <p>Raises:</p> Type Description <code>AppException</code> <p>If model is not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def get_document_topics(self) -&gt; List[Dict[str, Union[int, float]]]:\n    \"\"\"\n    Returns the dominant topic for each document along with topic distribution.\n\n    Returns:\n        List of dicts containing dominant_topic, probability, and full distribution.\n\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.document_topic_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        document_topics = []\n\n        for doc_idx, topic_dist in enumerate(self.document_topic_matrix):\n            # Normalize to get probabilities\n            topic_probs = topic_dist / (topic_dist.sum() + 1e-10)\n            dominant_topic = int(np.argmax(topic_probs))\n            dominant_prob = float(topic_probs[dominant_topic])\n\n            document_topics.append({\n                \"document_id\": doc_idx,\n                \"dominant_topic\": dominant_topic,\n                \"dominant_probability\": round(dominant_prob, 4),\n                \"topic_distribution\": [round(float(p), 4) for p in topic_probs]\n            })\n\n        self.logger.info(\n            f\"Extracted dominant topics for {len(document_topics)} documents.\"\n        )\n        return document_topics\n\n    except Exception as e:\n        self.logger.error(f\"Failed to extract document topics: {e}\")\n        raise AppException(f\"Document topic extraction failed: {str(e)}\")\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model.NMFModeler.compute_coherence","title":"src.topics.nmf_model.NMFModeler.compute_coherence","text":"<pre><code>compute_coherence(texts: List[List[str]], measure: str = 'c_v', num_words: int = 10) -&gt; float\n</code></pre> <p>Compute coherence score for the extracted topics using Gensim.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[List[str]]</code> <p>List of tokenized documents (list of list of strings).</p> required <code>measure</code> <code>str</code> <p>Coherence measure ('c_v', 'u_mass', 'c_uci', 'c_npmi').</p> <code>'c_v'</code> <code>num_words</code> <code>int</code> <p>Number of top words per topic to use for coherence.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>Coherence score (float).</p> <p>Raises:</p> Type Description <code>AppException</code> <p>On error or if model not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def compute_coherence(\n    self,\n    texts: List[List[str]],\n    measure: str = 'c_v',\n    num_words: int = 10\n) -&gt; float:\n    \"\"\"\n    Compute coherence score for the extracted topics using Gensim.\n\n    Args:\n        texts: List of tokenized documents (list of list of strings).\n        measure: Coherence measure ('c_v', 'u_mass', 'c_uci', 'c_npmi').\n        num_words: Number of top words per topic to use for coherence.\n\n    Returns:\n        Coherence score (float).\n\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None or self.topic_term_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        if not texts or not isinstance(texts, list):\n            raise DataValidationError(\"texts must be a non-empty list of tokenized documents.\")\n\n        # Create Gensim dictionary from texts\n        dictionary = Dictionary(texts)\n\n        # Extract top words for each topic\n        topic_words = []\n        for topic_idx in range(self.num_topics):\n            top_indices = self.topic_term_matrix[topic_idx].argsort()[-num_words:][::-1]\n            top_words = [self.feature_names[i] for i in top_indices]\n            topic_words.append(top_words)\n\n        # Compute coherence using Gensim\n        coherence_model = CoherenceModel(\n            topics=topic_words,\n            texts=texts,\n            dictionary=dictionary,\n            coherence=measure\n        )\n\n        coherence_score = coherence_model.get_coherence()\n\n        self.logger.info(\n            f\"Coherence score ({measure}): {coherence_score:.4f}\"\n        )\n\n        return coherence_score\n\n    except DataValidationError as e:\n        self.logger.error(f\"Data validation error in coherence computation: {e}\")\n        raise\n    except Exception as e:\n        self.logger.error(f\"Failed to compute coherence score: {e}\")\n        raise AppException(f\"Coherence computation failed: {str(e)}\")\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model.NMFModeler.reduce_dimensions_pca","title":"src.topics.nmf_model.NMFModeler.reduce_dimensions_pca","text":"<pre><code>reduce_dimensions_pca(n_components: int = 2) -&gt; np.ndarray\n</code></pre> <p>Apply PCA to document-topic matrix for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of principal components (default: 2 for 2D viz).</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Reduced dimensional representation of documents.</p> <p>Raises:</p> Type Description <code>AppException</code> <p>If model is not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def reduce_dimensions_pca(\n    self,\n    n_components: int = 2\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply PCA to document-topic matrix for visualization.\n\n    Args:\n        n_components: Number of principal components (default: 2 for 2D viz).\n\n    Returns:\n        Reduced dimensional representation of documents.\n\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.document_topic_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        self.logger.info(\n            f\"Applying PCA to reduce dimensions to {n_components} components...\"\n        )\n\n        pca = PCA(n_components=n_components, random_state=self.random_state)\n        reduced_data = pca.fit_transform(self.document_topic_matrix)\n\n        explained_variance = pca.explained_variance_ratio_\n        self.logger.info(\n            f\"PCA complete. Explained variance: \"\n            f\"{', '.join([f'{v:.2%}' for v in explained_variance])}\"\n        )\n\n        return reduced_data\n\n    except Exception as e:\n        self.logger.error(f\"PCA dimensionality reduction failed: {e}\")\n        raise AppException(f\"PCA failed: {str(e)}\")\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model.NMFModeler.visualize_topic_heatmap","title":"src.topics.nmf_model.NMFModeler.visualize_topic_heatmap","text":"<pre><code>visualize_topic_heatmap(num_words: int = 10, figsize: Tuple[int, int] = (12, 8), save_path: Optional[str] = None) -&gt; None\n</code></pre> <p>Visualize topic-term matrix as a heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>num_words</code> <code>int</code> <p>Number of top words per topic to display.</p> <code>10</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size (width, height).</p> <code>(12, 8)</code> <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the figure.</p> <code>None</code> <p>Raises:</p> Type Description <code>AppException</code> <p>If model is not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def visualize_topic_heatmap(\n    self,\n    num_words: int = 10,\n    figsize: Tuple[int, int] = (12, 8),\n    save_path: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Visualize topic-term matrix as a heatmap.\n\n    Args:\n        num_words: Number of top words per topic to display.\n        figsize: Figure size (width, height).\n        save_path: Optional path to save the figure.\n\n    Raises:\n        AppException: If model is not trained.\n    \"\"\"\n    try:\n        if self.model is None or self.topic_term_matrix is None:\n            raise AppException(\"NMF model is not trained yet.\")\n\n        if self.feature_names is None:\n            raise AppException(\"Feature names not available.\")\n\n        self.logger.info(\"Creating topic-term heatmap visualization...\")\n\n        # Get top words for each topic\n        top_words_per_topic = []\n        heatmap_data = []\n\n        for topic_idx in range(self.num_topics):\n            top_indices = self.topic_term_matrix[topic_idx].argsort()[-num_words:][::-1]\n            top_words = [self.feature_names[i] for i in top_indices]\n            top_words_per_topic.extend(top_words)\n\n            # Extract weights for these words\n            weights = [self.topic_term_matrix[topic_idx][i] for i in top_indices]\n            heatmap_data.append(weights)\n\n        # Remove duplicates while preserving order\n        unique_words = []\n        seen = set()\n        for word in top_words_per_topic:\n            if word not in seen:\n                unique_words.append(word)\n                seen.add(word)\n\n        # Create heatmap data matrix\n        heatmap_matrix = np.zeros((self.num_topics, len(unique_words)))\n        word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n\n        for topic_idx in range(self.num_topics):\n            top_indices = self.topic_term_matrix[topic_idx].argsort()[-num_words:][::-1]\n            for word_idx in top_indices:\n                word = self.feature_names[word_idx]\n                if word in word_to_idx:\n                    heatmap_matrix[topic_idx, word_to_idx[word]] = \\\n                        self.topic_term_matrix[topic_idx][word_idx]\n\n        # Create heatmap\n        plt.figure(figsize=figsize)\n        sns.heatmap(\n            heatmap_matrix,\n            xticklabels=unique_words,\n            yticklabels=[f\"Topic {i}\" for i in range(self.num_topics)],\n            cmap=\"YlOrRd\",\n            cbar_kws={'label': 'Weight'},\n            linewidths=0.5\n        )\n\n        plt.title(\"Topic-Term Heatmap\", fontsize=16, fontweight='bold')\n        plt.xlabel(\"Top Terms\", fontsize=12)\n        plt.ylabel(\"Topics\", fontsize=12)\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n\n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            self.logger.info(f\"Heatmap saved to: {save_path}\")\n        else:\n            plt.show()\n\n        plt.close()\n\n    except Exception as e:\n        self.logger.error(f\"Heatmap visualization failed: {e}\")\n        raise AppException(f\"Visualization failed: {str(e)}\")\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model.NMFModeler.save_model","title":"src.topics.nmf_model.NMFModeler.save_model","text":"<pre><code>save_model(path: str) -&gt; None\n</code></pre> <p>Saves the trained NMF model and associated data to a file using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the model.</p> required <p>Raises:</p> Type Description <code>AppException</code> <p>On error or if model not trained.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def save_model(self, path: str) -&gt; None:\n    \"\"\"\n    Saves the trained NMF model and associated data to a file using joblib.\n\n    Args:\n        path: Path to save the model.\n\n    Raises:\n        AppException: On error or if model not trained.\n    \"\"\"\n    try:\n        if self.model is None:\n            raise AppException(\"NMF model is not trained and cannot be saved.\")\n\n        model_data = {\n            'model': self.model,\n            'feature_names': self.feature_names,\n            'document_topic_matrix': self.document_topic_matrix,\n            'topic_term_matrix': self.topic_term_matrix,\n            'num_topics': self.num_topics,\n            'init': self.init,\n            'max_iter': self.max_iter,\n            'random_state': self.random_state,\n            'alpha': self.alpha,\n            'l1_ratio': self.l1_ratio\n        }\n\n        joblib.dump(model_data, path)\n        self.logger.info(f\"NMF model saved to: {path}\")\n\n    except Exception as e:\n        self.logger.error(f\"Failed to save NMF model: {e}\")\n        raise AppException(f\"Model save failed: {str(e)}\")\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model.NMFModeler.load_model","title":"src.topics.nmf_model.NMFModeler.load_model","text":"<pre><code>load_model(path: str) -&gt; None\n</code></pre> <p>Loads an NMF model from the given path using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to load the model from.</p> required <p>Raises:</p> Type Description <code>AppException</code> <p>On error loading the model.</p> Source code in <code>src\\topics\\nmf_model.py</code> <pre><code>def load_model(self, path: str) -&gt; None:\n    \"\"\"\n    Loads an NMF model from the given path using joblib.\n\n    Args:\n        path: Path to load the model from.\n\n    Raises:\n        AppException: On error loading the model.\n    \"\"\"\n    try:\n        model_data = joblib.load(path)\n\n        self.model = model_data['model']\n        self.feature_names = model_data['feature_names']\n        self.document_topic_matrix = model_data['document_topic_matrix']\n        self.topic_term_matrix = model_data['topic_term_matrix']\n        self.num_topics = model_data['num_topics']\n        self.init = model_data['init']\n        self.max_iter = model_data['max_iter']\n        self.random_state = model_data['random_state']\n        self.alpha = model_data['alpha']\n        self.l1_ratio = model_data['l1_ratio']\n\n        self.logger.info(f\"NMF model loaded from: {path}\")\n        self.logger.info(f\"Model configuration: {self.num_topics} topics\")\n\n    except Exception as e:\n        self.logger.error(f\"Failed to load NMF model: {e}\")\n        raise AppException(f\"Model load failed: {str(e)}\")\n</code></pre>"},{"location":"api/topics/#src.topics.nmf_model-functions","title":"Functions","text":""},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#src.utils.logger","title":"src.utils.logger","text":""},{"location":"api/utils/#src.utils.logger-functions","title":"Functions","text":""},{"location":"api/utils/#src.utils.logger.setup_logging","title":"src.utils.logger.setup_logging","text":"<pre><code>setup_logging(log_level: str = 'INFO', log_file: Optional[str] = None, force: bool = False) -&gt; None\n</code></pre> <p>Sets up logging with both console and rotating file handlers. Creates logs/ folder if not existing. If <code>force</code> is True, reconfigures handlers even if already initialized.</p> Source code in <code>src\\utils\\logger.py</code> <pre><code>def setup_logging(log_level: str = \"INFO\", log_file: Optional[str] = None, force: bool = False) -&gt; None:\n    \"\"\"\n    Sets up logging with both console and rotating file handlers.\n    Creates logs/ folder if not existing.\n    If `force` is True, reconfigures handlers even if already initialized.\n    \"\"\"\n    global _initialized\n    if _initialized and not force:\n        return\n\n    logger = logging.getLogger()\n    logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n\n    # Clear existing handlers when forcing reconfiguration\n    if force:\n        for h in list(logger.handlers):\n            logger.removeHandler(h)\n\n    formatter = logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT)\n\n    # Console handler\n    ch = logging.StreamHandler()\n    ch.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    # Rotating file handler\n    # Determine log file path: explicit arg -&gt; env -&gt; default\n    log_file_path = (\n        Path(os.environ.get(\"LOG_FILE_PATH\")) if os.environ.get(\"LOG_FILE_PATH\") else None\n    )\n    if log_file and isinstance(log_file, str):\n        log_file_path = Path(log_file)\n    if log_file_path is None:\n        log_file_path = DEFAULT_LOG_FILE\n\n    log_file_path.parent.mkdir(exist_ok=True, parents=True)\n\n    fh = RotatingFileHandler(str(log_file_path), maxBytes=5*1024*1024, backupCount=5, encoding='utf-8')\n    fh.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n\n    _initialized = True\n</code></pre>"},{"location":"api/utils/#src.utils.logger.get_logger","title":"src.utils.logger.get_logger","text":"<pre><code>get_logger(name: Optional[str] = None) -&gt; logging.Logger\n</code></pre> <p>Returns a logger instance with the given name. Ensures logging is configured. Usage: logger = get_logger(name)</p> Source code in <code>src\\utils\\logger.py</code> <pre><code>def get_logger(name: Optional[str] = None) -&gt; logging.Logger:\n    \"\"\"\n    Returns a logger instance with the given name. Ensures logging is configured.\n    Usage: logger = get_logger(__name__)\n    \"\"\"\n    setup_logging()\n    return logging.getLogger(name)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/installation/#install-dependencies","title":"Install Dependencies","text":"<p>Install the required packages for MkDocs documentation:</p> <pre><code>pip install mkdocs\npip install mkdocs-material\npip install mkdocstrings[python]\npip install mkdocs-gen-files\npip install mkdocs-literate-nav\npip install mkdocs-git-revision-date-localized-plugin\n</code></pre>"},{"location":"getting-started/installation/#install-project-dependencies","title":"Install Project Dependencies","text":"<p>Install the project's core dependencies:</p> <pre><code>pip install gensim\npip install scikit-learn\npip install pandas\npip install numpy\npip install matplotlib\npip install seaborn\npip install wordcloud\npip install pyLDAvis\npip install bertopic\npip install transformers\npip install torch\n</code></pre>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>For development, install additional tools:</p> <pre><code>pip install pytest\npip install black\npip install flake8\npip install mypy\npip install pre-commit\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test that everything is working:</p> <pre><code># Test MkDocs\nmkdocs --version\n\n# Test project imports\npython -c \"from src.topics.lda_model import LDAModeler; print('Installation successful!')\"\n</code></pre>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with topic modeling in just a few minutes.</p>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quick-start/#1-prepare-your-data","title":"1. Prepare Your Data","text":"<pre><code>from src.data.data_preprocessing import TextPreprocessor\n\n# Your raw texts\nraw_texts = [\n    \"This is a sample document about machine learning.\",\n    \"Another document discussing artificial intelligence.\",\n    \"A third document about data science and analytics.\"\n]\n\n# Initialize preprocessor\npreprocessor = TextPreprocessor()\n\n# Preprocess texts\nprocessed_texts = preprocessor.preprocess_texts(raw_texts)\n</code></pre>"},{"location":"getting-started/quick-start/#2-create-topic-model","title":"2. Create Topic Model","text":"<pre><code>from src.topics.lda_model import LDAModeler\nfrom src.features.tfidf import TFIDFVectorizer\n\n# Vectorize texts\nvectorizer = TFIDFVectorizer()\ncorpus, dictionary = vectorizer.fit_transform(processed_texts)\n\n# Train LDA model\nlda = LDAModeler(num_topics=5)\nlda.train(corpus, dictionary)\n\n# Get topics\ntopics = lda.get_topics(num_words=10)\nfor topic in topics:\n    print(f\"Topic {topic['topic_id']}: {topic['words']}\")\n</code></pre>"},{"location":"getting-started/quick-start/#3-evaluate-model","title":"3. Evaluate Model","text":"<pre><code># Compute coherence score\ncoherence_score = lda.compute_coherence_score(processed_texts, dictionary)\nprint(f\"Coherence Score: {coherence_score:.4f}\")\n\n# Compute perplexity\nperplexity = lda.compute_perplexity(corpus)\nprint(f\"Perplexity: {perplexity:.4f}\")\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Data Preprocessing</li> <li>Explore Topic Modeling in detail</li> <li>Check out Model Evaluation techniques</li> <li>Browse the API Reference for complete documentation</li> </ul>"},{"location":"user-guide/data-preprocessing/","title":"Data Preprocessing","text":"<p>The data preprocessing module provides tools for cleaning and preparing text data for topic modeling.</p>"},{"location":"user-guide/data-preprocessing/#textpreprocessor-class","title":"TextPreprocessor Class","text":"<p>The <code>TextPreprocessor</code> class handles the cleaning and preprocessing of raw text data.</p>"},{"location":"user-guide/data-preprocessing/#key-features","title":"Key Features","text":"<ul> <li>Text Cleaning: Removes special characters, numbers, and extra whitespace</li> <li>Tokenization: Splits text into individual words</li> <li>Stopword Removal: Removes common words that don't carry meaning</li> <li>Lemmatization: Reduces words to their base form</li> <li>Customizable: Configurable preprocessing steps</li> </ul>"},{"location":"user-guide/data-preprocessing/#usage-example","title":"Usage Example","text":"<pre><code>from src.data.data_preprocessing import TextPreprocessor\n\n# Initialize preprocessor\npreprocessor = TextPreprocessor(\n    remove_stopwords=True,\n    lemmatize=True,\n    min_word_length=3\n)\n\n# Process texts\nraw_texts = [\"Your raw text data here...\"]\nprocessed_texts = preprocessor.preprocess_texts(raw_texts)\n</code></pre>"},{"location":"user-guide/data-preprocessing/#data-ingestion","title":"Data Ingestion","text":"<p>The <code>DataIngestion</code> class handles loading data from various sources.</p>"},{"location":"user-guide/data-preprocessing/#supported-formats","title":"Supported Formats","text":"<ul> <li>CSV files</li> <li>JSON files</li> <li>Text files</li> <li>Database connections</li> </ul>"},{"location":"user-guide/data-preprocessing/#example","title":"Example","text":"<pre><code>from src.data.data_ingestion import DataIngestion\n\n# Load data from CSV\ningestion = DataIngestion()\ndata = ingestion.load_csv(\"path/to/data.csv\", text_column=\"content\")\n</code></pre>"},{"location":"user-guide/model-evaluation/","title":"Model Evaluation","text":"<p>This guide covers the evaluation metrics and techniques available for assessing topic model quality.</p>"},{"location":"user-guide/model-evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"user-guide/model-evaluation/#coherence-score","title":"Coherence Score","text":"<p>The coherence score measures the semantic similarity between words in a topic.</p> <pre><code># Compute coherence score\ncoherence_score = lda.compute_coherence_score(\n    texts=processed_texts,\n    id2word=dictionary,\n    coherence='c_v'  # Options: 'c_v', 'c_uci', 'c_npmi'\n)\n</code></pre> <p>Interpretation: - Higher scores indicate better topic quality - Values typically range from -1 to 1 - c_v is generally preferred for most use cases</p>"},{"location":"user-guide/model-evaluation/#perplexity","title":"Perplexity","text":"<p>Perplexity measures how well the model predicts unseen data.</p> <pre><code># Compute perplexity\nperplexity = lda.compute_perplexity(corpus)\n</code></pre> <p>Interpretation: - Lower perplexity indicates better model fit - Should be evaluated on held-out test data - Can be used to compare different models</p>"},{"location":"user-guide/model-evaluation/#model-comparison","title":"Model Comparison","text":""},{"location":"user-guide/model-evaluation/#comparing-lda-and-nmf","title":"Comparing LDA and NMF","text":"<pre><code>from src.topics.lda_model import LDAModeler\nfrom src.topics.nmf_model import NMFModeler\n\n# Train both models\nlda = LDAModeler(num_topics=10)\nnmf = NMFModeler(num_topics=10)\n\nlda.train(corpus, dictionary)\nnmf.train(corpus, dictionary)\n\n# Compare metrics\nlda_coherence = lda.compute_coherence_score(texts, dictionary)\nnmf_coherence = nmf.compute_coherence_score(texts, dictionary)\n\nprint(f\"LDA Coherence: {lda_coherence:.4f}\")\nprint(f\"NMF Coherence: {nmf_coherence:.4f}\")\n</code></pre>"},{"location":"user-guide/model-evaluation/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code># Test different numbers of topics\ntopic_counts = [5, 10, 15, 20]\ncoherence_scores = []\n\nfor num_topics in topic_counts:\n    lda = LDAModeler(num_topics=num_topics)\n    lda.train(corpus, dictionary)\n    coherence = lda.compute_coherence_score(texts, dictionary)\n    coherence_scores.append(coherence)\n    print(f\"Topics: {num_topics}, Coherence: {coherence:.4f}\")\n</code></pre>"},{"location":"user-guide/model-evaluation/#best-practices","title":"Best Practices","text":"<ol> <li>Use Multiple Metrics: Combine coherence and perplexity</li> <li>Cross-Validation: Evaluate on held-out data</li> <li>Parameter Tuning: Test different hyperparameters</li> <li>Domain Knowledge: Validate topics manually</li> <li>Reproducibility: Set random seeds for consistent results</li> </ol>"},{"location":"user-guide/topic-modeling/","title":"Topic Modeling","text":"<p>This guide covers the topic modeling capabilities of the project, including LDA and NMF implementations.</p>"},{"location":"user-guide/topic-modeling/#lda-latent-dirichlet-allocation","title":"LDA (Latent Dirichlet Allocation)","text":"<p>The <code>LDAModeler</code> class provides a comprehensive interface for LDA topic modeling.</p>"},{"location":"user-guide/topic-modeling/#key-features","title":"Key Features","text":"<ul> <li>Flexible Configuration: Customizable hyperparameters</li> <li>Model Evaluation: Coherence and perplexity metrics</li> <li>Topic Extraction: Get top words for each topic</li> <li>Document Classification: Assign topics to documents</li> <li>Model Persistence: Save and load trained models</li> </ul>"},{"location":"user-guide/topic-modeling/#basic-usage","title":"Basic Usage","text":"<pre><code>from src.topics.lda_model import LDAModeler\n\n# Initialize model\nlda = LDAModeler(\n    num_topics=10,\n    passes=10,\n    chunksize=100,\n    alpha='auto',\n    random_state=42\n)\n\n# Train model\nlda.train(corpus, dictionary)\n\n# Get topics\ntopics = lda.get_topics(num_words=10)\n</code></pre>"},{"location":"user-guide/topic-modeling/#model-evaluation","title":"Model Evaluation","text":"<pre><code># Compute coherence score\ncoherence = lda.compute_coherence_score(texts, dictionary)\n\n# Compute perplexity\nperplexity = lda.compute_perplexity(corpus)\n\n# Get dominant topics per document\ndominant_topics = lda.get_dominant_topic_per_doc(corpus)\n</code></pre>"},{"location":"user-guide/topic-modeling/#nmf-non-negative-matrix-factorization","title":"NMF (Non-negative Matrix Factorization)","text":"<p>The <code>NMFModeler</code> class provides NMF-based topic modeling.</p>"},{"location":"user-guide/topic-modeling/#usage","title":"Usage","text":"<pre><code>from src.topics.nmf_model import NMFModeler\n\n# Initialize model\nnmf = NMFModeler(\n    num_topics=10,\n    max_iter=1000,\n    random_state=42\n)\n\n# Train model\nnmf.train(corpus, dictionary)\n\n# Get topics\ntopics = nmf.get_topics()\n</code></pre>"},{"location":"user-guide/topic-modeling/#choosing-between-lda-and-nmf","title":"Choosing Between LDA and NMF","text":"<ul> <li>LDA: Better for longer documents, probabilistic approach</li> <li>NMF: Better for shorter documents, deterministic approach</li> <li>Evaluation: Use coherence scores to compare models</li> </ul>"}]}